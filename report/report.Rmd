---
title: "Bayesian Data Analysis Report on Titanic Dataset"
author: "Alp Gunsever"
date: "13/01/2021"
output: pdf_document
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = FALSE)
```

## 1 - Introduction 

This is a data analysis report intended to test bayesian data analysis skills using the titanic dataset from Kaggle. A bayesian data analysis framework will be followed to perform a full analysis on the titanic dataset. The analysis will be finalized by coming up with a final model and making a prediction with it in the Kaggle competition. All the results and observations will be shared and explained as much as possible for the whole process.

```{r libraries, cache=TRUE, eval=TRUE, echo=FALSE, message=FALSE}
library(stringr)
library(corrplot)
library(projpred)
library(bayesplot)
library(mice)
library(VIM)
library(ggplot2)
theme_set(bayesplot::theme_default(base_family = "sans"))
library(dplyr)
library(gridExtra)
library(brms)
library(pROC)
```

## 2- Exploratory Data Analysis

It will be beneficial to look into the raw data available before getting into any analysis work. However, to be able to observe the data at hand, the existing observations for different predictors will be formatted into data structures that can be handled by R in a meaningful way.

The following adjustments have been implemented to the training and test data sets together after they are combined into a single dataset:

* Passenger class predictor is transformed into factor type with classes "1", "2" and "3" set as separate levels.
* Titles have been extracted from the name variable and factored into "Mr", "Mrs", "Miss", "Master", "Noble" and "Soldier" levels based on the implications of various titles.
* Cabin predictor is transformed into factor type according to the letter in the cabin name.
* Embarked predictor is transformed into factor type with the first letters of the ports representing different levels as "C", "Q" and "S".
* Ticket predictor is factored into the number of digits of the number part of the ticket.
* Family size predictor is created based on number of siblings and spouses including self. Family type predictor is created based on the family size predictor. If the total number of family members are equal to 1, then family type is assigned to "Singleton". If family size is between 1 and 4, then family type is assigned to "small". If family size is greater than 4, then family type is assigned to "large". Family type is factored into these 3 levels.
* Sex predictor is changed to isMale and factored as a binary predictor with 1 indicating a male passenger and 0 a female passenger.

```{r clean data, cache=TRUE, echo=FALSE}
# read train and test data
dir <- file.path("C:","Users","alpgu","Dropbox","Data Science","Bayesian Data Analysis","BDA3 (Gelman)","titanic","data")
dat.train <- read.csv(file.path(dir,"train.csv"))
dat.test <- read.csv(file.path(dir,"test.csv"))

# combine train and test data for cleaning
dat.train_without_survival <- cbind(data.frame(PassengerId=dat.train[,1]),dat.train[,3:length(colnames(dat.train))])
dat.full <- rbind(dat.train_without_survival, dat.test)

# adjustments on data set

# factorize Pclass variable
dat.full$Pclass <- as.factor(dat.full$Pclass)

# extract titles from names
dat.full$title <- str_split_fixed(as.character(str_split_fixed(dat.full$Name, ",", 2)[,2]),". ",2)[,1]
dat.full$title <- gsub(" ", "", dat.full$title, fixed = TRUE)
dat.full$title[760] <- "Countess"
for (i in 1:length(dat.full$title)){
  if(grepl("Mr|Mrs|Miss|Master",dat.full$title[i]) == F){
    if(grepl("Countess|Sir|Lady|Don",dat.full$title[i]) == T){
      dat.full$title[i] <- "Noble"
    }
    else if(grepl("Capt|Col|Major",dat.full$title[i]) == T){
      dat.full$title[i] <- "Soldier"
    } 
    else{
      if(dat.full$Sex[i] == "male"){
        dat.full$title[i] <- "Mr"  
      } else{
        dat.full$title[i] <- "Miss"
      }
    }
  } 
}
dat.full$title <- as.factor(dat.full$title)

# factorize cabin variable according to letter
dat.full$Cabin <-sapply(dat.full$Cabin, function(x) substr(x,1,1))
dat.full$Cabin[dat.full$Cabin==""] <- NA
dat.full$Cabin <- as.factor(dat.full$Cabin)

# factorize embarked variable according to letter
dat.full$Embarked[dat.full$Embarked==""] <- NA
dat.full$Embarked <- as.factor(dat.full$Embarked)

# reduce factor size of ticket variable
for (i in 1:length(dat.full$Ticket)) {
  if (length(strsplit(dat.full$Ticket[i], " ")[[1]]) == 2) {
    dat.full$Ticket[i] <- strsplit(dat.full$Ticket[i], " ")[[1]][2]  
  } else{
    dat.full$Ticket[i] <- strsplit(dat.full$Ticket[i], " ")[[1]][1]   
  }
  
}

dat.full$Ticket <- sapply(dat.full$Ticket, function(x) ifelse(nchar(x)>1,as.character(nchar(x)),x))

#for (i in 1:length(dat.full$Ticket)) {
#   if (dat.full$Ticket[i] == '2' || dat.full$Ticket[i] == '3' || dat.full$Ticket[i] == '7'){
#     dat.full$Ticket[i] <- "Other"
#   }
#}

dat.full$Ticket <- as.factor(dat.full$Ticket)

# Create family size variable including the passenger itself
dat.full <- transform(dat.full, 'familySize' =  SibSp + Parch +1)
dat.full <- dat.full[ , -which(names(dat.full) %in% c("SibSp","Parch"))]
dat.full$fSize <- NA
dat.full$fSize[dat.full$familySize == 1] <- 'singleton'
dat.full$fSize[dat.full$familySize <= 4 & dat.full$familySize > 1] <- 'small'
dat.full$fSize[dat.full$familySize > 4] <- 'large'
dat.full <- dat.full[ , -which(names(dat.full) %in% c("familySize"))]
dat.full$fSize <- as.factor(dat.full$fSize)

# Change sex variable to more understandable isMale variable
dat.full$isMale <- NA
dat.full$isMale[dat.full$Sex == "male"] <- 1
dat.full$isMale[dat.full$Sex <= "female"] <- 0
dat.full$isMale <- as.factor(dat.full$isMale)
dat.full <- dat.full[ , -which(names(dat.full) %in% c("Sex"))]

# Deleting the names variable
dat.full <- dat.full[ , -which(names(dat.full) %in% c("Name"))]
```

The data summary for training and test sets combined can be seen below:

```{r data summary1, cache=TRUE, echo=FALSE}
summary(dat.full)
```

Age predictor has 263 missing observations, fare has 1, cabin has 1014 and embarked has 2 out of a total of 1309 observations. It seems still possible to replace the missing observations even in Age predictor but imputing the cabin predictor can bring a lot of noise to the data. So the cabin predictor might be dropped but the rest of the predictors will be definitely imputed. However, before getting to that part, it will be best to look at the data summary below showing the structure of each predictor:

```{r data summary2, cache=TRUE, echo=FALSE}
str(dat.full)
```

Another step to take before getting into data imputation is to look at the available data visually. However, to keep the test set information seperate, the exploratory visual checks will be performed only on training set. As a starting point to that, the first plot on below left shows the average fee paid for the ticket of each title. The one on the right shows the number of observations for each title:

```{r visual check with available data1, cache=TRUE, echo=FALSE, fig.height = 3}
dat.full.train <- data.frame(Survived = as.factor(dat.train[,2]),dat.full[1:891,])
g <- ggplot()
title_avFare <- dat.full.train %>% group_by(title) %>% summarise(n=n(),avFare = mean(Fare, na.rm = T))
p1 <- g + geom_col(data = title_avFare,aes(x=title,y=avFare),fill = "deepskyblue3")
p2 <- g + geom_col(data = title_avFare,aes(x=title,y=n),fill = "salmon3")
grid.arrange(p1,p2, ncol=2)
```

The average fee for the title make sense with Noble title having the highest average fee and Mr title  having the lowest most probably because of the contribution of single male passengers that have entered the ship with a cheap ticket. The next two graphs below show the relationship between these titles and the survival rate. The one on the left shows the total number of counts represented by coloured bars with green representing the number of survived passengers and red representing the ones that haven't survived the accident. The normalized version showing the proportion of survived and not survived passengers for each title is shown on the below right graph:

```{r visual check with available data2, cache=TRUE, echo=FALSE, fig.height = 3}
survtitle1 <- g + geom_bar(data = dat.full.train, aes(x=title, fill = Survived), position = "stack") + theme(axis.text.x=element_text(angle = 45, vjust = 1))
survtitle2 <- g + geom_bar(data = dat.full.train, aes(x=title, fill = Survived), position = "fill") + theme(axis.text.x=element_text(angle = 45, vjust = 1))
grid.arrange(survtitle1,survtitle2, ncol=2)
```

It can be seen that men with cheaper tickets have lower survival rates in comparison to the other titles on the ship. Young passengers, female passengers and nobles seem to have a higher survival rate. Next graph below shows the average fare per ticket type whereas the one on the right shows the number of observations for each ticket type:

```{r visual check with available data3, cache=TRUE, echo=FALSE, fig.height = 3}
ticket_avFare <- dat.full.train %>% group_by(Ticket) %>% summarise(n=n(),avFare = mean(Fare, na.rm = T))
t1 <- g + geom_col(data = ticket_avFare,aes(x=Ticket,y=avFare),fill = "deepskyblue3")
t2 <- g + geom_col(data = ticket_avFare,aes(x=Ticket,y=n),fill = "salmon3")
grid.arrange(t1,t2, ncol=2)
```

It can be seen that the average ticket prices are similar between types 3 and 7 and between types 4 and 6. Ticket type 5 is highest compared to other ticket types. On the other hand, types 3 and 7 are owned by lowest number of passengers on the ship whereas type 6 is owned most of the passengers. As mentioned before, the visual exploratory checks are performed only on the training set. However in the summary table above, there was also a ticket type 2 which is coming from the test set. It will be hard to make a prediction about this ticket type if we can't model it on the training set. There are 3 passengers that have ticket type 2, 14 passengers with ticket type 3 and 46 passengers with ticket type 7. Based on the assumption that the average fee paid for these ticket types are similar, it might make sense to combine these three ticket types before modeling. The next two graphs below show the relationship between these ticket types and the survival rate. The one on the left shows the total number of counts represented by coloured bars with green representing the number of survived passengers and red representing the ones that haven't survived the accident. The normalized version showing the proportion of survived and not survived passengers for each ticket type is shown on the below right graph:

```{r visual check with available data4, cache=TRUE, echo=FALSE, fig.height = 3}
survTicket1 <- g + geom_bar(data = dat.full.train, aes(x=Ticket, fill = Survived), position = "stack")
survTicket2 <- g + geom_bar(data = dat.full.train, aes(x=Ticket, fill = Survived), position = "fill")
grid.arrange(survTicket1,survTicket2, ncol=2)
```

It is a bit hard to make separate conclusions for each ticket type but an obvious one is that the passengers having expensive ticket types have higher survival rates. Next graph below shows the average fare per passenger class whereas the one on the right shows the number of observations for each passenger class:

```{r visual check with available data5, cache=TRUE, echo=FALSE, fig.height = 3}
Pclass_avFare <- dat.full.train %>% group_by(Pclass) %>% summarise(n=n(),avFare = mean(Fare, na.rm = T))
pc1 <- g + geom_col(data = Pclass_avFare,aes(x=Pclass,y=avFare),fill = "deepskyblue3")
pc2 <- g + geom_col(data = Pclass_avFare,aes(x=Pclass,y=n),fill = "salmon3")
grid.arrange(pc1,pc2, ncol=2)
```

As expected, class 1 passengers pay the highest price compared to other passenger classes. It can also be observed that Class 3 passengers have the highest number. The interesting observation is that class 2 passengers are less in amount in comparison to class 1 passengers. The next two graphs below show the relationship between these passenger classes and the survival rate. The one on the left shows the total number of counts represented by coloured bars with green representing the number of survived passengers and red representing the ones that haven't survived the accident. The normalized version showing the proportion of survived and not survived passengers for each passenger class is shown on the below right graph:   
```{r visual check with available data6, cache=TRUE, echo=FALSE, fig.height = 3}
survClass1 <- g + geom_bar(data = dat.full.train, aes(x=Pclass, fill = Survived), position = "stack")
survClass2 <- g + geom_bar(data = dat.full.train, aes(x=Pclass, fill = Survived), position = "fill")
grid.arrange(survClass1,survClass2, ncol=2)
```

The survival rate is proportional to the passenger class with passengers that pay the highest price survive the most proportionally. Next we will check the cabin predictor in the same manner.

```{r visual check with available data7, cache=TRUE, echo=FALSE, fig.height = 3}
cabin_avFare <- dat.full.train %>% group_by(Cabin) %>% summarise(n=n(),avFare = mean(Fare, na.rm = T))
cab1 <- g + geom_col(data = cabin_avFare,aes(x=Cabin,y=avFare),fill = "deepskyblue3")
cab2 <- g + geom_col(data = cabin_avFare,aes(x=Cabin,y=n),fill = "salmon3")
grid.arrange(cab1,cab2, ncol=2)
```

As mentioned before, the number of missing values for the cabin predictor is very high. This means either the cabin predictor will be dropped from analysis, which will result in loss of information, or a logic will be applied for imputation of the missing values. The following graphs show the relationship of the cabin predictor to survival rate:

```{r visual check with available data8, cache=TRUE, echo=FALSE, fig.height = 3}
survCabin1 <- g + geom_bar(data = dat.full.train, aes(x=Cabin, fill = Survived), position = "stack")
survCabin2 <- g + geom_bar(data = dat.full.train, aes(x=Cabin, fill = Survived), position = "fill")
grid.arrange(survCabin1,survCabin2, ncol=2)
```

There is not a particular pattern that can be noticed immediately but again cabins with passengers that have paid higher ticket fees tend to have higher survival rate. Next we will check the family size predictor in similar fashion to other predictors above:

```{r visual check with available data9, cache=TRUE, echo=FALSE, fig.height = 3}
fSize_avFare <- dat.full.train %>% group_by(fSize) %>% summarise(n=n(),avFare = mean(Fare, na.rm = T))
f1 <- g + geom_col(data = fSize_avFare,aes(x=fSize,y=avFare),fill = "deepskyblue3")
f2 <- g + geom_col(data = fSize_avFare,aes(x=fSize,y=n),fill = "salmon3")
grid.arrange(f1,f2, ncol=2)
```

Large families have higher average fare but less in total number. The highest number of individuals are single ones whereas small families also make up a big part of the total number of passengers. The next two graphs show the family size relationship to survival rate:

```{r visual check with available data10, cache=TRUE, echo=FALSE, fig.height = 3}
survfSize1 <- g + geom_bar(data = dat.full.train, aes(x=fSize, fill = Survived), position = "stack")
survfSize2 <- g + geom_bar(data = dat.full.train, aes(x=fSize, fill = Survived), position = "fill")
grid.arrange(survfSize1,survfSize2, ncol=2)
```

It can be seen that large families and single passengers don't have high survival rate whereas small families have a higher survival rate. Still, the total number of survived single passengers and small families are very close to each other. The next predictor that is going to be observed is the embarkation port:

```{r visual check with available data11, cache=TRUE, echo=FALSE, fig.height = 3}
embarked_avFare <- dat.full.train %>% group_by(Embarked) %>% summarise(n=n(),avFare = mean(Fare, na.rm = T))
em1 <- g + geom_col(data = embarked_avFare,aes(x=Embarked,y=avFare),fill = "deepskyblue3")
em2 <- g + geom_col(data = embarked_avFare,aes(x=Embarked,y=n),fill = "salmon3")
grid.arrange(em1,em2, ncol=2)
```

The figure on the upper left shows the average fare for passengers that embarked from different ports. The 2 missing values have the highest rate. The passengers that embarked from port S are the highest in number. Next, the relationship to survival rate will be checked:

```{r visual check with available data12, cache=TRUE, echo=FALSE, fig.height = 3}
survEmbarked1 <- g + geom_bar(data = dat.full.train, aes(x=Embarked, fill = Survived), position = "stack")
survEmbarked2 <- g + geom_bar(data = dat.full.train, aes(x=Embarked, fill = Survived), position = "fill")
grid.arrange(survEmbarked1,survEmbarked2, ncol=2)
```

It can again be seen that passengers that have paid the highest rate for their tickets in average embarked from a similar port and have a higher survival rate. We will check the sex variable below in similar fashion although it doesn't totally make sense to check the ticket fee for each sex. We will do it for the sake of keeping the same format but will add more specific plots for the predictors:

```{r visual check with available data13, cache=TRUE, echo=FALSE, fig.height = 3}
sex_avFare <- dat.full.train %>% group_by(isMale) %>% summarise(n=n(),avFare = mean(Fare, na.rm = T))
sex1 <- g + geom_col(data = sex_avFare,aes(x=isMale,y=avFare),fill = "deepskyblue3")
sex2 <- g + geom_col(data = sex_avFare,aes(x=isMale,y=n),fill = "salmon3")
grid.arrange(sex1,sex2, ncol=2)
```

Interestingly, it looks like female passengers have paid more in average for their tickets. There are more male passengers on board which might show the indication that the male passengers are part of a lower passenger class. We will check the relationship to the survival rate now:

```{r visual check with available data14, cache=TRUE, echo=FALSE, fig.height = 3}
survSex1 <- g + geom_bar(data = dat.full.train, aes(x=isMale, fill = Survived), position = "stack")
survSex2 <- g + geom_bar(data = dat.full.train, aes(x=isMale, fill = Survived), position = "fill")
grid.arrange(survSex1,survSex2, ncol=2)
```

It can be easily noticed that the survival rate is high among female passengers. This predictor is highly correlated to the suvival rate.

We've checked all the categorical variables in relation to the average fee and survival rate. However, it doesn't actually make total sense to check all of the categorical variables in relation to average ticket fee such as cabin type or titles. The following graphs will try to add more meaningful visual checks for the available data.

First we will check the relationship between the cabin and the family size:

```{r visual check with available data15, cache=TRUE, echo=FALSE, fig.height = 3}
cabFsize1 <- g + geom_bar(data = dat.full.train, aes(x=Cabin, fill = fSize), position = "stack")
cabFsize2 <- g + geom_bar(data = dat.full.train, aes(x=Cabin, fill = fSize), position = "fill")
grid.arrange(cabFsize1,cabFsize2, ncol=2)
```

The number of missing values are so high that the graph on the left above doesn't give much information. Data imputation is necessary to be able to get something meaningful out of the above plot. We will get back to this plot and more others after the imputation is finalized in the next section. However, before that let's have a look into the continous predictor fare for tickets. Age predictor has a lot of missing observations so we will look at it after the imputation as well.

```{r visual check with available data16, cache=TRUE, echo=FALSE, fig.height = 3, message=FALSE}
fare1 <- g + geom_histogram(data = dat.full.train, aes(Fare), color = "black", fill = "deepskyblue3")
fare2 <- g + geom_boxplot(data = dat.full.train, aes(Survived, Fare), color = "black", fill = "salmon3")
grid.arrange(fare1,fare2, ncol=2)
```

The fare predictor has a skewed distribution so a log transformation or normalization might be necessary. 

### 2.1 Data Imputation

We will impute the missing age observations by replacing them with median of each title group that the individual belongs to.

```{r visual check with available data17, cache=TRUE, eval=FALSE, echo=FALSE}
#Imputing the age variable
for (i in 1:length(dat.full$Age)){
  if(is.na(dat.full$Age[i])==T){
    if(dat.full$title[i] == "Master") dat.full$Age[i] = title_avAge[1,4]
    if(dat.full$title[i] == "Miss") dat.full$Age[i] = title_avAge[2,4]
    if(dat.full$title[i] == "Mr") dat.full$Age[i] = title_avAge[3,4]
    if(dat.full$title[i] == "Mrs") dat.full$Age[i] = title_avAge[4,4]
    if(dat.full$title[i] == "Noble") dat.full$Age[i] = title_avAge[5,4]
    if(dat.full$title[i] == "Soldier") dat.full$Age[i] = title_avAge[6,4]
  }
}
dat.full$Age <- as.numeric(dat.full$Age)

# Making title more balanced
dat.full$title <- as.character(dat.full$title)
for (i in 1:length(dat.full$title)){
  if(grepl("Master|Noble|Soldier",dat.full$title[i]) == T){
    dat.full$title[i] <- "Other"  
  } 
}
dat.full$title <- as.factor(dat.full$title)
dat.full$title <- droplevels(dat.full$title)

dat.full.train <- data.frame(Survived = as.factor(dat.train[,2]),dat.full[1:891,])
g + geom_boxplot(data = dat.full.train, aes(Survived, Age))
```

Individuals that are upper class passengers, are members of small families(2 to 4 family members), embarked from C and are below 60 years old tend to have higher survival rates. 

Cabin variable will be dropped as it doesn't seem possible to impute this predictor with lots of missing observations. However, it might still makes sense to impute age predictor.

```{r missing data, cache=TRUE, eval=FALSE, echo=FALSE}
dat.full <- dat.full[ , !(names(dat.full) %in% c("Cabin"))]
md.pattern(dat.full, rotate.names = TRUE)

imp_full <- mice(dat.full, m = 20, maxit = 40,printFlag = FALSE)
summary(imp_full)
imp_full$method
plot(imp_full)
dat.full_imp <- complete(imp_full)

# Log transform continuous age and fare variable
dat.full_imp$FareLog <- log(dat.full_imp$Fare+1) # 1 added to avoid log(0)
dat.full_imp$AgeLog <- log(dat.full_imp$Age+1) # 1 added to avoid log(0)
dat.full_imp <- dat.full_imp[ , -which(names(dat.full_imp) %in% c("Age","Fare"))]

train_imp <- data.frame(PassengerId = dat.full_imp[1:891,1], Survived = as.factor(dat.train[,2]), dat.full_imp[1:891,2:9])
test_imp <- dat.full_imp[892:1309,]
```

The imputed dataset will be used for analysis.

```{r model input, cache=TRUE, eval=FALSE, echo=FALSE}
y.train <- as.numeric(train_imp$Survived) # outcome
y.train <- y.train-1
pId.train <- train_imp$PassengerId # ID for predictions
pId.test <- test_imp$PassengerId # ID for predictions

predictor.variables <- colnames(train_imp)[3:length(colnames(train_imp))]
predictor1.variables <- predictor.variables[predictor.variables != "title"]
predictor2.variables <- predictor.variables[predictor.variables != "Pclass"]
predictor3.variables <- predictor.variables[!predictor.variables %in% c("Ticket","Embarked")]
predictor4.variables <- predictor.variables[!predictor.variables %in% c("Pclass","isMale")]
predictor5.variables <- predictor.variables[!predictor.variables %in% c("Pclass","title")]

formula_train <- as.formula(y.train~ .)
formula_test <- as.formula(~ .)

X <- subset(train_imp, select=predictor.variables)
X0.train <- model.matrix(formula_train, data = X)[,-1]
X <- subset(test_imp, select=predictor.variables)
X0.test <- model.matrix(formula_test, data = X)[,-1]

X <- subset(train_imp, select=predictor1.variables)
X1.train <- model.matrix(formula_train, data = X)[,-1]
X <- subset(test_imp, select=predictor1.variables)
X1.test <- model.matrix(formula_test, data = X)[,-1]

X <- subset(train_imp, select=predictor2.variables)
X2.train <- model.matrix(formula_train, data = X)[,-1]
X <- subset(test_imp, select=predictor2.variables)
X2.test <- model.matrix(formula_test, data = X)[,-1]

X <- subset(train_imp, select=predictor3.variables)
X3.train <- model.matrix(formula_train, data = X)[,-1]
X <- subset(test_imp, select=predictor3.variables)
X3.test <- model.matrix(formula_test, data = X)[,-1]

X <- subset(train_imp, select=predictor4.variables)
X4.train <- model.matrix(formula_train, data = X)[,-1]
X <- subset(test_imp, select=predictor4.variables)
X4.test <- model.matrix(formula_test, data = X)[,-1]

X <- subset(train_imp, select=predictor5.variables)
X5.train <- model.matrix(formula_train, data = X)[,-1]
X <- subset(test_imp, select=predictor5.variables)
X5.test <- model.matrix(formula_test, data = X)[,-1]

colnames(X0.train) <- gsub(" ", "", colnames(X0.train), fixed = TRUE)
colnames(X0.test) <- gsub(" ", "", colnames(X0.test), fixed = TRUE)
colnames(X1.train) <- gsub(" ", "", colnames(X1.train), fixed = TRUE)
colnames(X1.test) <- gsub(" ", "", colnames(X1.test), fixed = TRUE)
colnames(X2.train) <- gsub(" ", "", colnames(X2.train), fixed = TRUE)
colnames(X2.test) <- gsub(" ", "", colnames(X2.test), fixed = TRUE)
colnames(X3.train) <- gsub(" ", "", colnames(X3.train), fixed = TRUE)
colnames(X3.test) <- gsub(" ", "", colnames(X3.test), fixed = TRUE)
colnames(X4.train) <- gsub(" ", "", colnames(X4.train), fixed = TRUE)
colnames(X4.test) <- gsub(" ", "", colnames(X4.test), fixed = TRUE)
colnames(X5.train) <- gsub(" ", "", colnames(X5.train), fixed = TRUE)
colnames(X5.test) <- gsub(" ", "", colnames(X5.test), fixed = TRUE)

X0.train <- scale(X0.train)
X0.test <- scale(X0.test)
X1.train <- scale(X1.train)
X1.test <- scale(X1.test)
X2.train <- scale(X2.train)
X2.test <- scale(X2.test)
X3.train <- scale(X3.train)
X3.test <- scale(X3.test)
X4.train <- scale(X4.train)
X4.test <- scale(X4.test)
X5.train <- scale(X5.train)
X5.test <- scale(X5.test)

titleTrain <- as.numeric(train_imp$title)
titleTest <- as.numeric(test_imp$title)

pClassTrain <- as.numeric(train_imp$Pclass)
pClassTest <- as.numeric(test_imp$Pclass)

ticketTrain <- as.numeric(train_imp$Ticket)
ticketTest <- as.numeric(test_imp$Ticket)

embarkedTrain <- as.numeric(train_imp$Embarked)
embarkedTest <- as.numeric(test_imp$Embarked)

isMaleTrain <- as.numeric(train_imp$isMale)
isMaleTest <- as.numeric(test_imp$isMale)
```

## 3- Prior Predictive Checking

We will perform a prior predictive check with only using the priors and no data.The reason for making prior predictive analysis is to make a sanity check on the priors without using the likelihood. 

```{r prior predictive checking, cache=TRUE, eval=FALSE, echo=FALSE}
# Prior predictive checks
df0.train <- as.data.frame(cbind(y.train, X0.train))

title <- titleTrain
df1.train <- as.data.frame(cbind(y.train, X1.train,title))

pClass <- pClassTrain
df2.train <- as.data.frame(cbind(y.train, X2.train,pClass))

ticket <- ticketTrain
embarked <- embarkedTrain
df3.train <- as.data.frame(cbind(y.train, X3.train,ticket,embarked))

isMale <- isMaleTrain
df4.train <- as.data.frame(cbind(y.train,X4.train,pClass,isMale))

df5.train <- as.data.frame(cbind(y.train,X5.train,pClass,title))

formula_only_intercept_population <- as.formula(y.train ~ 1)
formula_only_intercept_population <- bf(formula = formula_only_intercept_population, center = F)

formula_only_intercept_population_and_group <- as.formula(y.train ~ 1 + (1|title))
formula_only_intercept_population_and_group <- bf(formula = formula_only_intercept_population_and_group, center = F)

formula_all_population <- as.formula(paste("y.train ~", paste0("1+",paste(colnames(df0.train)[2:length(colnames(df0.train))], collapse = "+"))))
formula_all_population <- bf(formula = formula_all_population, center = F)

formula_hier1_title <- as.formula(paste("y.train ~", paste0("1+",paste(colnames(df1.train)[2:(length(colnames(df1.train))-1)], collapse = "+")),"+(1+",paste(colnames(df1.train)[2:(length(colnames(df1.train))-1)], collapse = "+"),"|title)"))
formula_hier1_title <- bf(formula = formula_hier1_title, center = F)

formula_hier2_pClass <- as.formula(paste("y.train ~", paste0("1+",paste(colnames(df2.train)[2:(length(colnames(df2.train))-1)], collapse = "+")),"+(1+",paste(colnames(df2.train)[2:(length(colnames(df2.train))-1)], collapse = "+"),"|pClass)"))
formula_hier2_pClass <- bf(formula = formula_hier2_pClass, center = F)

prior0 <- prior(student_t(3,0,2.5), class = b)
prior1 <- prior(student_t(3,0,2.5), class = b) + prior(lkj(2), class = cor)
prior2 <- prior(horseshoe(1), class = b)

multi_hier1_title_prior.fit <- brm(formula = formula_hier1_title,
                                       data = df1.train,
                                       prior = prior1,
                                       sample_prior = "only",
                                       family = bernoulli(link="logit"),
                                       cores = parallel::detectCores(),
                                       control = list(adapt_delta = 0.98, max_treedepth = 15))

yrep.prior1 <- posterior_predict(multi_hier1_title_prior.fit)

# Graphical checks
pp_check(y.train, yrep.prior1[1:50,], ppc_dens_overlay)
pp_check(y.train, yrep.prior1, fun = "stat_grouped", group = train_imp$title, stat = "mean")
```

We have used weakly informative robust prior of student_t(3,0,2.5) for both population-level and group-level parameters. We also used lkj(2) for the correlation matrix. It can be seen that the spread for prior predictive samples have much higher variance than the actual response variable for the training set. It confirms that the chosen prior can be used for posterior predictive check along with the likelihood.

## 4 - Model Fitting and Algorithm Diagnostics
```{r model fitting, cache=TRUE, eval=FALSE, echo=FALSE}
df0.test <- as.data.frame(cbind(X0.test))

title <- titleTest
df1.test <- as.data.frame(cbind(X1.test,title))

pClass <- pClassTest
df2.test <- as.data.frame(cbind(X2.test,pClass))

# model 1: Only intercept model at population level----------------------------------------------------
only_population_intercept.fit <- brm(formula = formula_only_intercept_population,
                                      data = df1.train,
                                      prior = prior0,
                                      family = bernoulli(link="logit"),
                                      cores = parallel::detectCores(),
                                      control = list(adapt_delta = 0.98, max_treedepth = 15),
                                      save_pars = save_pars(all = TRUE),
                                      seed = 187)

# model 2: All predictors model with population effects only------------------------------------------
population_all.fit <- brm(formula = formula_all_population,
                                      data = df0.train,
                                      prior = prior0,
                                      family = bernoulli(link="logit"),
                                      cores = parallel::detectCores(),
                                      control = list(adapt_delta = 0.98, max_treedepth = 15),
                                      save_pars = save_pars(all = TRUE),
                                      seed = 187)

# model 3: All predictors model with population effects only and horseshoe prior----------------------
population_all_hs.fit <- brm(formula = formula_all_population,
                                      data = df0.train,
                                      prior = prior2,
                                      family = bernoulli(link="logit"),
                                      cores = parallel::detectCores(),
                                      control = list(adapt_delta = 0.98, max_treedepth = 15),
                                      save_pars = save_pars(all = TRUE),
                                      seed = 187)

# model 4: Only intercept model at population level and grouping level by title------------------------
population_and_group_intercept.fit <- brm(formula = formula_only_intercept_population_and_group,
                                      data = df1.train,
                                      prior = prior0,
                                      family = bernoulli(link="logit"),
                                      cores = parallel::detectCores(),
                                      control = list(adapt_delta = 0.98, max_treedepth = 15),
                                      save_pars = save_pars(all = TRUE),
                                      seed = 187)

# model 5: All predictors model with population and group effects by title----------------------------
multi_hier1_title_post.fit <- brm(formula = formula_hier1_title,
                                       data = df1.train,
                                       prior = prior1,
                                       family = bernoulli(link="logit"),
                                       cores = parallel::detectCores(),
                                       control = list(adapt_delta = 0.98, max_treedepth = 15),
                                       save_pars = save_pars(all = TRUE),
                                       seed = 187)

# model 6: All predictors model with population and group effects by pClass----------------------------
multi_hier2_pClass_post.fit <- brm(formula = formula_hier2_pClass,
                                       data = df2.train,
                                       prior = prior1,
                                       family = bernoulli(link="logit"),
                                       cores = parallel::detectCores(),
                                       control = list(adapt_delta = 0.98, max_treedepth = 15),
                                       save_pars = save_pars(all = TRUE),
                                       seed = 187)
```


```{r simulating posterior predictions, cache=TRUE, eval=FALSE, echo=FALSE}
# making predictions
y_pred1 <- posterior_predict(only_population_intercept.fit, df0.test)
y_pred2 <- posterior_predict(population_all.fit, df0.test)
y_pred3 <- posterior_predict(population_all_hs.fit, df0.test)
y_pred4 <- posterior_predict(population_and_group_intercept.fit, df1.test)
y_pred5 <- posterior_predict(multi_hier1_title_post.fit, df1.test)
y_pred6 <- posterior_predict(multi_hier2_pClass_post.fit, df2.test)

yrep1.posterior <- posterior_predict(only_population_intercept.fit)
yrep2.posterior <- posterior_predict(population_all.fit)
yrep3.posterior <- posterior_predict(population_all_hs.fit)
yrep4.posterior <- posterior_predict(population_and_group_intercept.fit)
yrep5.posterior <- posterior_predict(multi_hier1_title_post.fit)
yrep6.posterior <- posterior_predict(multi_hier2_pClass_post.fit)
```

## 5 - Posterior Predictive Checking
```{r posterior predictive checking, cache=TRUE, eval=FALSE, echo=FALSE}
summary(only_population_intercept.fit)
summary(population_all.fit)
summary(population_all_hs.fit)
summary(population_and_group_intercept.fit)
summary(multi_hier1_title_post.fit)
summary(multi_hier2_pClass_post.fit)

plot(conditional_effects(population_all.fit), ask = F)
plot(conditional_effects(multi_hier1_title_post.fit), ask = F)

pp_check(y.train, yrep1.posterior[1:50,], ppc_dens_overlay)
pp_check(y.train, yrep2.posterior[1:50,], ppc_dens_overlay)
pp_check(y.train, yrep3.posterior[1:50,], ppc_dens_overlay)
pp_check(y.train, yrep4.posterior[1:50,], ppc_dens_overlay)
pp_check(y.train, yrep5.posterior[1:50,], ppc_dens_overlay)
pp_check(y.train, yrep6.posterior[1:50,], ppc_dens_overlay)

pp_check(y.train, yrep5.posterior, fun = "stat_grouped", group = train_imp$title, stat = "mean")
pp_check(y.train, yrep6.posterior, fun = "stat_grouped", group = train_imp$pClass, stat = "mean")

mcmc_areas(population_all_hs.fit, pars = c("b_Intercept",paste0("b_",colnames(df1.train)[2:(length(colnames(df1.train))-1)])))

mcmc_hist(multi_hier1_title_post.fit, pars = c("b_Intercept",paste0("b_",colnames(df1.train)[2:(length(colnames(df1.train))-1)])))

# posterior classification accuracy for model5
pred5 <- colMeans(yrep5.posterior)
pr5 <- as.integer(pred5 >= 0.5)
model5_pca <- round(mean(xor(pr5,as.integer(y.train==0))),2)

# posterior balanced classification accuracy for model5
model5_bpca <- round((mean(xor(pr5[y.train==0]>0.5,as.integer(y.train[y.train==0])))+mean(xor(pr5[y.train==1]<0.5,as.integer(y.train[y.train==1]))))/2,2)
```

## 6 - Additional Models and Model Improvements
```{r additional models, cache=TRUE, eval=FALSE, echo=FALSE}

# Interaction terms added to all predictors with population terms only model
formula_all_population_interactions <- as.formula(y.train ~ 1 + Pclass2 + Pclass3 + Ticket5 + Ticket6 + TicketC + TicketOther + TicketP + TicketS + EmbarkedQ + EmbarkedS + titleMr + titleMrs + titleOther + fSizesingleton + fSizesmall + isMale1 + FareLog + AgeLog + fSizesingleton:FareLog + Pclass3:AgeLog + FareLog:AgeLog + I(AgeLog^2))

formula_all_population_interactions <- bf(formula = formula_all_population_interactions, center = F)

population_all_interactions.fit <- brm(formula = formula_all_population_interactions,
                                      data = df0.train,
                                      prior = prior0,
                                      family = bernoulli(link="logit"),
                                      cores = parallel::detectCores(),
                                      control = list(adapt_delta = 0.98, max_treedepth = 15),
                                      save_pars = save_pars(all = TRUE),
                                      seed = 187)

# Interaction terms added to all predictors with hierarchical model grouped by title
formula_multi_hier1_title_interactions <- as.formula(y.train ~ 1 + Pclass2 + Pclass3 + Ticket5 + Ticket6 + TicketC + TicketOther + TicketP + TicketS + EmbarkedQ + EmbarkedS + fSizesingleton + fSizesmall + isMale1 + FareLog + AgeLog + fSizesingleton:FareLog + Pclass3:AgeLog + FareLog:AgeLog + I(AgeLog^2) + (1 + Pclass2 + Pclass3 + Ticket5 + Ticket6 + TicketC + TicketOther + TicketP + TicketS + EmbarkedQ + EmbarkedS + fSizesingleton + fSizesmall + isMale1 + FareLog + AgeLog | title))

formula_multi_hier1_title_interactions <- bf(formula = formula_multi_hier1_title_interactions, center = F)

multi_hier1_title_post_interactions.fit <- brm(formula = formula_multi_hier1_title_interactions,
                                       data = df1.train,
                                       prior = prior1,
                                       family = bernoulli(link="logit"),
                                       cores = parallel::detectCores(),
                                       control = list(adapt_delta = 0.98, max_treedepth = 15),
                                       save_pars = save_pars(all = TRUE),
                                       seed = 187)

# Interaction terms added to all predictors with hierarchical model grouped by pClass
formula_multi_hier2_pClass_interactions <- as.formula(y.train ~ 1 + Ticket5 + Ticket6 + TicketC + TicketOther + TicketP + TicketS + EmbarkedQ + EmbarkedS + titleMr + titleMrs + titleOther + fSizesingleton + fSizesmall + isMale1 + FareLog + AgeLog + fSizesingleton:FareLog + titleOther:AgeLog + FareLog:AgeLog + I(AgeLog^2) + (1 + Ticket5 + Ticket6 + TicketC + TicketOther + TicketP + TicketS + EmbarkedQ + EmbarkedS + titleMr + titleMrs + titleOther + fSizesingleton + fSizesmall + isMale1 + FareLog + AgeLog | pClass))

formula_multi_hier2_pClass_interactions <- bf(formula = formula_multi_hier2_pClass_interactions, center = F)

multi_hier2_pClass_post_interactions.fit <- brm(formula = formula_multi_hier2_pClass_interactions,
                                       data = df2.train,
                                       prior = prior1,
                                       family = bernoulli(link="logit"),
                                       cores = parallel::detectCores(),
                                       control = list(adapt_delta = 0.98, max_treedepth = 15),
                                       save_pars = save_pars(all = TRUE),
                                       seed = 187)

# Double grouping for hierarchical model with title and embarked
formula_hier3_doubleClass <- as.formula(paste("y.train ~", paste0("1+",paste(colnames(df3.train)[2:(length(colnames(df3.train))-2)], collapse = "+")),"+(1+",paste(colnames(df3.train)[2:(length(colnames(df3.train))-2)], collapse = "+"),"|(ticket:embarked))"))
formula_hier3_doubleClass <- bf(formula = formula_hier3_doubleClass, center = F)

multi_hier3_doubleClass.fit <- brm(formula = formula_hier3_doubleClass,
                                      data = df3.train,
                                      prior = prior1,
                                      family = bernoulli(link="logit"),
                                      cores = parallel::detectCores(),
                                      control = list(adapt_delta = 0.98, max_treedepth = 15),
                                      save_pars = save_pars(all = TRUE),
                                      seed = 187)

# Double grouping for hierarchical model with pClass and isMale
formula_hier4_doubleClass <- as.formula(paste("y.train ~", paste0("1+",paste(colnames(df4.train)[2:(length(colnames(df4.train))-2)], collapse = "+")),"+(1+",paste(colnames(df4.train)[2:(length(colnames(df4.train))-2)], collapse = "+"),"|(pClass:isMale))"))
formula_hier4_doubleClass <- bf(formula = formula_hier4_doubleClass, center = F)

multi_hier4_doubleClass.fit <- brm(formula = formula_hier4_doubleClass,
                                      data = df4.train,
                                      prior = prior1,
                                      family = bernoulli(link="logit"),
                                      cores = parallel::detectCores(),
                                      control = list(adapt_delta = 0.98, max_treedepth = 15),
                                      save_pars = save_pars(all = TRUE),
                                      seed = 187)

# Double grouping for hierarchical model with fare variable and grouped by pClass and title
formula_hier5_doubleClass <- as.formula(y.train ~ 1 + FareLog + (1 + FareLog | pClass:title))
formula_hier5_doubleClass <- bf(formula = formula_hier5_doubleClass, center = F)

multi_hier5_doubleClass.fit <- brm(formula = formula_hier5_doubleClass,
                                      data = df5.train,
                                      prior = prior1,
                                      family = bernoulli(link="logit"),
                                      cores = parallel::detectCores(),
                                      control = list(adapt_delta = 0.98, max_treedepth = 15),
                                      save_pars = save_pars(all = TRUE),
                                      seed = 187)
```

## 7 - Model Comparison
```{r model comparison, cache=TRUE, eval=FALSE, echo=FALSE}
loo1 <- brms::loo(only_population_intercept.fit, save_psis = TRUE, cores = 4, reloo = T)
loo2 <- brms::loo(population_all.fit, save_psis = TRUE, cores = 4, reloo = T)
loo3 <- brms::loo(population_all_hs.fit, save_psis = TRUE, cores = 4, reloo = T)
loo4 <- brms::loo(population_and_group_intercept.fit, save_psis = TRUE, cores = 4, reloo = T)
loo5 <- brms::loo(multi_hier1_title_post.fit, save_psis = TRUE, cores = 4, reloo = T)
loo6 <- brms::loo(multi_hier2_pClass_post.fit, save_psis = TRUE, cores = 4, reloo = T)
loo7 <- brms::loo(population_all_interactions.fit, save_psis = TRUE, cores = 4, reloo = T)
loo8 <- brms::loo(multi_hier1_title_post_interactions.fit, save_psis = TRUE, cores = 4, reloo = T)
loo9 <- brms::loo(multi_hier2_pClass_post_interactions.fit, save_psis = TRUE, cores = 4, reloo = T)
loo10 <- brms::loo(multi_hier3_doubleClass.fit, save_psis = TRUE, cores = 4, reloo = T)
loo11 <- brms::loo(multi_hier4_doubleClass.fit, save_psis = TRUE, cores = 4, reloo = T)
loo12 <- brms::loo(multi_hier5_doubleClass.fit, save_psis = TRUE, cores = 4, reloo = T)

print(loo1)
print(loo2)
print(loo3)
print(loo4)
print(loo5)
print(loo6)
print(loo7)
print(loo8)
print(loo9)
print(loo10)
print(loo11)
print(loo12)

plot(loo1, label_points = TRUE)
plot(loo2, label_points = TRUE)
plot(loo3, label_points = TRUE)
plot(loo4, label_points = TRUE)
plot(loo5, label_points = TRUE)
plot(loo6, label_points = TRUE)
plot(loo7, label_points = TRUE)
plot(loo8, label_points = TRUE)
plot(loo9, label_points = TRUE)
plot(loo10, label_points = TRUE)
plot(loo11, label_points = TRUE)
plot(loo12, label_points = TRUE)

brms::loo_compare(loo1,loo2,loo3,loo4,loo5,loo6,loo7,loo8,loo9,loo10,loo11,loo12)

# using list of loo objects
loo_list <- list(loo1,loo2,loo3,loo4,loo5,loo6,loo7,loo8,loo9,loo10, loo11,loo12)
loo_model_weights(loo_list)
```

## 8 - Prediction Submission
```{r predictions model1, cache=TRUE, eval=FALSE, echo=FALSE}
title <- titleTest
pClass <- pClassTest
ticket <- ticketTest
embarked <- embarkedTest
isMale <- isMaleTest

df.preds <- cbind(df0.test,title,pClass,ticket,embarked,isMale)

probs_all <- brms::pp_average(only_population_intercept.fit,population_all.fit,population_all_hs.fit,
                          population_and_group_intercept.fit,multi_hier1_title_post.fit,
                          multi_hier2_pClass_post.fit,population_all_interactions.fit,
                          multi_hier1_title_post_interactions.fit,multi_hier2_pClass_post_interactions.fit,
                          multi_hier3_doubleClass.fit,multi_hier4_doubleClass.fit,multi_hier5_doubleClass.fit,
                          newdata = df.preds, allow_new_levels = T)

preds_all <- ifelse(probs_all[,1] > 0.5, 1, 0)

model_all.submission <- data.frame("PassengerId" = pId.test, "Survived" = preds_all)
write.csv(x = model_all.submission, file = file.path("C:","Users","alpgu","Dropbox","Data Science",                                             "Bayesian Data Analysis","BDA3 (Gelman)","titanic","submissions","model_bma.csv"),
          row.names = FALSE)

# prediction with best predictive performance model according to loo
ticket <- ticketTest
embarked <- embarkedTest
df3.test <- as.data.frame(cbind(X3.test,ticket,embarked))

y_pred10<- posterior_predict(multi_hier3_doubleClass.fit, df3.test, allow_new_levels = T)
prob10 <- colMeans(y_pred10)
pred10 <- as.integer(prob10 >= 0.5)

model_doubleClass.submission <- data.frame("PassengerId" = pId.test, "Survived" = pred10)
write.csv(x = model_doubleClass.submission, file = file.path("C:","Users","alpgu","Dropbox","Data Science",                                             "Bayesian Data Analysis","BDA3 (Gelman)","titanic","submissions","model_doubleClass.csv"),
          row.names = FALSE)
```
