---
title: "titanic_report"
author: "Alp Gunsever"
date: "13/01/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## 1 - Introduction 

This is a data analysis report intended to test bayesian data analysis skills using the titanic dataset from kaggle.

```{r libraries}
library(rstan)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
library(shinystan)
library(stringr)
source(file.path("C:","Users","alpgu","Dropbox","Data Science","Bayesian Data Analysis","BDA3 (Gelman)","titanic","scripts","helperFunctions_temp.R"))
library(corrplot)
library(rstanarm)
library(projpred)
library(bayesplot)
theme_set(bayesplot::theme_default(base_family = "sans"))
```

## 2- Exploratory Data Analysis

We are first going to make an exploratory analysis on the data available at hand. We format the data beforehand in both training and test sets.

The existing observations for different predictors will be formatted into data structures that can be handled by R in a meaningful way. 

```{r clean data}
# read train data
dir <- file.path("C:","Users","alpgu","Dropbox","Data Science","Bayesian Data Analysis","BDA3 (Gelman)","titanic","data")
dat.train <- read.csv(file.path(dir,"train.csv"))
dat.test <- read.csv(file.path(dir,"test.csv"))

# adjustments on data set
train <- cleanTitanicData_temp(dat.train,'train')
test <- cleanTitanicData_temp(dat.test,'test')
```

Then the missing observations will be handled seperately.

```{r missing data}

```

```{r input for rstanarm model1}
y.train <- as.numeric(train$Survived) # outcome
y.train <- y.train-1
pId.train <- train$PassengerId # ID for predictions
pId.test <- test$PassengerId # ID for predictions

predictor1.variables <- colnames(train)[3:length(colnames(train))]
predictor1.variables <- predictor1.variables[predictor1.variables != "Ticket"]
predictor2.variables <- colnames(train)[3:length(colnames(train))]
predictor2.variables <- predictor2.variables[predictor2.variables != "title"]

X1 <- subset(train, select=predictor1.variables)
X1.train <- model.matrix(y.train~., X1)[,-1] #drop int
X1.train <- X1.train[,-12] # removing titleMr
X1 <- subset(test, select=predictor1.variables)
X1.test <- model.matrix(~., X1)[,-1] #drop int
X1.test <- X1.test[,-12] # removing titleMr

X2 <- subset(train, select=predictor2.variables)
X2.train <- model.matrix(y.train~., X2)[,-1] #drop int
X2 <- subset(test, select=predictor2.variables)
X2.test <- model.matrix(~., X2)[,-1] #drop int

colnames(X1.train) <- gsub(" ", "", colnames(X1.train), fixed = TRUE)
colnames(X1.test) <- gsub(" ", "", colnames(X1.test), fixed = TRUE)

colnames(X2.train) <- gsub(" ", "", colnames(X2.train), fixed = TRUE)
colnames(X2.test) <- gsub(" ", "", colnames(X2.test), fixed = TRUE)

N1.train <- nrow(X1.train)
K1.train <- ncol(X1.train)
N1.test <- nrow(X1.test)
K1.test <- ncol(X1.test)

N2.train <- nrow(X2.train)
K2.train <- ncol(X2.train)
N2.test <- nrow(X2.test)
K2.test <- ncol(X2.test)

for(i in 1:K1.train){
  if (colnames(X1.train)[i] != 'AgeLog' & colnames(X1.train)[i] != 'FareLog') {
    X1.train[,i] <- scale(X1.train[,i], scale = F)
  } else{
    X1.train[,i] <- (X1.train[,i]-mean(X1.train[,i]))/(2*sd(X1.train[,i]))
  }
}

for(i in 1:K1.test){
  if (colnames(X1.test)[i] != 'AgeLog' & colnames(X1.test)[i] != 'FareLog') {
    X1.test[,i] <- scale(X1.test[,i], scale = F)
  } else{
    X1.test[,i] <- (X1.test[,i]-mean(X1.test[,i]))/(2*sd(X1.test[,i]))
  }
}

for(i in 1:K2.train){
  if (colnames(X2.train)[i] != 'AgeLog' & colnames(X2.train)[i] != 'FareLog') {
    X2.train[,i] <- scale(X2.train[,i], scale = F)
  } else{
    X2.train[,i] <- (X2.train[,i]-mean(X2.train[,i]))/(2*sd(X2.train[,i]))
  }
}

for(i in 1:K2.test){
  if (colnames(X2.test)[i] != 'AgeLog' & colnames(X2.test)[i] != 'FareLog') {
    X2.test[,i] <- scale(X2.test[,i], scale = F)
  } else{
    X2.test[,i] <- (X2.test[,i]-mean(X2.test[,i]))/(2*sd(X2.test[,i]))
  }
}

ticketTrain <- as.numeric(train$Ticket)
ticketTest <- as.numeric(test$Ticket)

titleTrain <- as.numeric(train$title)
titleTest <- as.numeric(test$title)
```

## 3- Prior Predictive Checking

## 4 - Model Fitting and Algorithm Diagnostics
```{r model1 fitting}
# model 1 ------------------------------------------------------------------------------------
ticket <- ticketTrain 
df1.train <- as.data.frame(cbind(y.train, X1.train,ticket))
ticket <- ticketTest
df1.test <- as.data.frame(cbind(X1.test,ticket))

title <- titleTrain 
df2.train <- as.data.frame(cbind(y.train, X2.train,title))
title <- titleTest
df2.test <- as.data.frame(cbind(X2.test,title))

formula1 <- as.formula(paste("y.train ~", paste0("1+",paste(colnames(X1.train), collapse = "+")),"+(1+",paste(colnames(X1.train), collapse = "+"),"|ticket)"))

model1_fit_hier_ticket <- stan_glmer(formula = formula1,
                              data = df1.train, 
                              cores=4,
                              adapt_delta = 0.98,
                              algorithm = "sampling",
                              family = binomial(link = "logit"),
                              prior = student_t(7,0,2.5),
                              prior_intercept = student_t(7,0,2.5))

# model 2 ------------------------------------------------------------------------------------
formula2 <- as.formula(paste("y.train ~", paste0("1+",paste(colnames(X2.train), collapse = "+")),"+(1+",paste(colnames(X2.train), collapse = "+"),"|title)"))

model2_fit_hier_ticket <- stan_glmer(formula = formula2,
                              data = df2.train, 
                              cores=4,
                              adapt_delta = 0.98,
                              algorithm = "sampling",
                              family = binomial(link = "logit"),
                              prior = student_t(7,0,2.5),
                              prior_intercept = student_t(7,0,2.5))

```

```{r model diagnostics}
summary(model1_fit_hier_ticket)
prior_summary(object = model1_fit_hier_ticket)
prior_summary(object = model2_fit_hier_ticket)

y_pred1.hier <- posterior_predict(model1_fit_hier_ticket, df1.test, draws = 500)
y_pred2.hier <- posterior_predict(model2_fit_hier_ticket, df2.test, draws = 500)

loo1.pred.hier <- loo(model1_fit_hier_ticket)
loo2.pred.hier <- loo(model2_fit_hier_ticket)

loo_compare(loo1.pred.hier,loo2.pred.hier)

ppc_dens_overlay(y.train, y_pred1.hier)
ppc_dens_overlay(y.train, y_pred2.hier)

mcmc_areas(as.matrix(model1_fit_hier_ticket)[,1:19])
mcmc_areas(as.matrix(model2_fit_hier_ticket)[,1:22])
```

## 5 - Posterior Predictive Checking

## 6 - Model Comparison

## 7 - Prediction Submission
```{r predictions model1}
probs1.hier <- apply(y_pred1.hier, 2, mean)
preds1.hier <- ifelse(probs1.hier > .5, 1, 0)

probs2.hier <- apply(y_pred2.hier, 2, mean)
preds2.hier <- ifelse(probs2.hier > .5, 1, 0)

model1_hier.submission <- data.frame("PassengerId" = pId.test, "Survived" = preds1.hier)
write.csv(x = model1_hier.submission, file = file.path("C:","Users","alpgu","Dropbox","Data Science",                                             "Bayesian Data Analysis","BDA3 (Gelman)","titanic","submissions","model1_hier_ticket.csv"),
          row.names = FALSE)

model2_hier.submission <- data.frame("PassengerId" = pId.test, "Survived" = preds2.hier)
write.csv(x = model2_hier.submission, file = file.path("C:","Users","alpgu","Dropbox","Data Science",                                             "Bayesian Data Analysis","BDA3 (Gelman)","titanic","submissions","model2_hier_title.csv"),
          row.names = FALSE)
```
