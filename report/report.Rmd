---
title: "Bayesian Data Analysis Report on Titanic Dataset"
author: "Alp Gunsever"
date: "13/01/2021"
output: pdf_document
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = FALSE)
```

## 1 - Introduction 

This is a data analysis report intended to test bayesian data analysis skills using the titanic dataset from Kaggle. A bayesian data analysis framework will be followed to perform a full analysis on the titanic dataset. The analysis will be finalized by coming up with a final model and making a prediction with it in the Kaggle competition. All the results and observations will be shared and explained as much as possible for the whole process.

```{r libraries, cache=TRUE, eval=TRUE, echo=FALSE, message=FALSE}
library(stringr)
library(corrplot)
library(projpred)
library(bayesplot)
library(mice)
library(VIM)
library(ggplot2)
theme_set(bayesplot::theme_default(base_family = "sans"))
library(dplyr)
library(gridExtra)
library(brms)
library(pROC)
```

## 2- Exploratory Data Analysis

It will be beneficial to look into the raw data available before getting into any analysis work. However, it must be stated that any comments on this section will be based on point averages and should be taken as initial observation not causal relationships. In order to do so, the existing observations for different predictors will be first formatted into data structures that can be handled by R in a meaningful way.

Before getting into any data cleaning or modification, let's have a look at some of the rows of the raw data:

```{r check raw data, cache=TRUE, echo=FALSE}
# read train and test data
dir <- file.path("C:","Users","alpgu","Dropbox","Data Science","Bayesian Data Analysis","BDA3 (Gelman)","titanic","data")
dat.train <- read.csv(file.path(dir,"train.csv"))
dat.test <- read.csv(file.path(dir,"test.csv"))

# combine train and test data for cleaning
dat.train_without_survival <- cbind(data.frame(PassengerId=dat.train[,1]),dat.train[,3:length(colnames(dat.train))])
dat.full <- rbind(dat.train_without_survival, dat.test)

head(dat.full)
```

The following adjustments have been implemented to the training and test data sets together after they are combined into a single dataset:

* Passenger class predictor is transformed into factor type with classes "1", "2" and "3" set as separate levels.
* Titles have been extracted from the name variable and factored into "Mr", "Mrs", "Miss", "Master", "Noble" and "Soldier" levels based on the implications of various titles.
* Cabin predictor is transformed into factor type according to the letter in the cabin name.
* Embarked predictor is transformed into factor type with the first letters of the ports representing different levels as "C", "Q" and "S".
* Ticket predictor is factored into the number of digits of the number part of the ticket.
* Family size predictor is created based on number of siblings and spouses including self. Family type predictor is created based on the family size predictor. If the total number of family members are equal to 1, then family type is assigned to "Singleton". If family size is between 1 and 4, then family type is assigned to "small". If family size is greater than 4, then family type is assigned to "large". Family type is factored into these 3 levels.
* Sex predictor is changed to isMale and factored as a binary predictor with 1 indicating a male passenger and 0 a female passenger.

```{r clean data, cache=TRUE, echo=FALSE}
# adjustments on data set

# factorize Pclass variable
dat.full$Pclass <- as.factor(dat.full$Pclass)

# extract titles from names
dat.full$title <- str_split_fixed(as.character(str_split_fixed(dat.full$Name, ",", 2)[,2]),". ",2)[,1]
dat.full$title <- gsub(" ", "", dat.full$title, fixed = TRUE)
dat.full$title[760] <- "Countess"
for (i in 1:length(dat.full$title)){
  if(grepl("Mr|Mrs|Miss|Master",dat.full$title[i]) == F){
    if(grepl("Countess|Sir|Lady|Don",dat.full$title[i]) == T){
      dat.full$title[i] <- "Noble"
    }
    else if(grepl("Capt|Col|Major",dat.full$title[i]) == T){
      dat.full$title[i] <- "Soldier"
    } 
    else{
      if(dat.full$Sex[i] == "male"){
        dat.full$title[i] <- "Mr"  
      } else{
        dat.full$title[i] <- "Miss"
      }
    }
  } 
}
dat.full$title <- as.factor(dat.full$title)

# factorize cabin variable according to letter
dat.full$Cabin <-sapply(dat.full$Cabin, function(x) substr(x,1,1))
dat.full$Cabin[dat.full$Cabin==""] <- NA
dat.full$Cabin <- as.factor(dat.full$Cabin)

# factorize embarked variable according to letter
dat.full$Embarked[dat.full$Embarked==""] <- NA
dat.full$Embarked <- as.factor(dat.full$Embarked)

# reduce factor size of ticket variable
for (i in 1:length(dat.full$Ticket)) {
  if (length(strsplit(dat.full$Ticket[i], " ")[[1]]) == 2) {
    dat.full$Ticket[i] <- strsplit(dat.full$Ticket[i], " ")[[1]][2]  
  } else{
    dat.full$Ticket[i] <- strsplit(dat.full$Ticket[i], " ")[[1]][1]   
  }
  
}

dat.full$Ticket <- sapply(dat.full$Ticket, function(x) ifelse(nchar(x)>1,as.character(nchar(x)),x))

#for (i in 1:length(dat.full$Ticket)) {
#   if (dat.full$Ticket[i] == '2' || dat.full$Ticket[i] == '3' || dat.full$Ticket[i] == '7'){
#     dat.full$Ticket[i] <- "Other"
#   }
#}

dat.full$Ticket <- as.factor(dat.full$Ticket)

# Create family size variable including the passenger itself
dat.full <- transform(dat.full, 'familySize' =  SibSp + Parch +1)
dat.full <- dat.full[ , -which(names(dat.full) %in% c("SibSp","Parch"))]
dat.full$fSize <- NA
dat.full$fSize[dat.full$familySize == 1] <- 'singleton'
dat.full$fSize[dat.full$familySize <= 4 & dat.full$familySize > 1] <- 'small'
dat.full$fSize[dat.full$familySize > 4] <- 'large'
dat.full <- dat.full[ , -which(names(dat.full) %in% c("familySize"))]
dat.full$fSize <- as.factor(dat.full$fSize)

# Change sex variable to more understandable isMale variable
dat.full$isMale <- NA
dat.full$isMale[dat.full$Sex == "male"] <- 1
dat.full$isMale[dat.full$Sex <= "female"] <- 0
dat.full$isMale <- as.factor(dat.full$isMale)
dat.full <- dat.full[ , -which(names(dat.full) %in% c("Sex"))]

# Deleting the names variable
dat.full <- dat.full[ , -which(names(dat.full) %in% c("Name"))]
```

The data summary for training and test sets combined can be seen below:

```{r data summary1, cache=TRUE, echo=FALSE}
summary(dat.full)
```

Age predictor has 263 missing observations, fare has 1, cabin has 1014 and embarked has 2 out of a total of 1309 observations. It seems still possible to replace the missing observations even in Age predictor but imputing the cabin predictor can bring a lot of noise to the data. So the cabin predictor might be dropped but the rest of the predictors will be definitely imputed. However, before getting to that part, it will be best to look at the data summary below showing the structure of each predictor:

```{r data summary2, cache=TRUE, echo=FALSE}
str(dat.full)
```

Another step to take before getting into data imputation is to look at the available data visually. However, to keep the test set information seperate, the exploratory visual checks will be performed only on training set. As a starting point to that, the first plot on below left shows the average fee paid for the ticket of each title. The one on the right shows the number of observations for each title:

```{r visual check with available data1, cache=TRUE, echo=FALSE, fig.height = 2.5}
dat.full.train <- data.frame(Survived = as.factor(dat.train[,2]),dat.full[1:891,])
g <- ggplot()
title_avFare <- dat.full.train %>% group_by(title) %>% summarise(n=n(),avFare = mean(Fare, na.rm = T))
p1 <- g + geom_col(data = title_avFare,aes(x=title,y=avFare),fill = "deepskyblue3")
p2 <- g + geom_col(data = title_avFare,aes(x=title,y=n),fill = "salmon3")
grid.arrange(p1,p2, ncol=2)
```

The average fee for the title make sense with Noble title having the highest average fee and Mr title  having the lowest most probably because of the contribution of single male passengers that have entered the ship with a cheap ticket. The next two graphs below show the relationship between these titles and the survival rate. The one on the left shows the total number of counts represented by coloured bars with green representing the number of survived passengers and red representing the ones that haven't survived the accident. The normalized version showing the proportion of survived and not survived passengers for each title is shown on the below right graph:

```{r visual check with available data2, cache=TRUE, echo=FALSE, fig.height = 2.5}
survtitle1 <- g + geom_bar(data = dat.full.train, aes(x=title, fill = Survived), position = "stack") + theme(axis.text.x=element_text(angle = 45, vjust = 1))
survtitle2 <- g + geom_bar(data = dat.full.train, aes(x=title, fill = Survived), position = "fill") + theme(axis.text.x=element_text(angle = 45, vjust = 1))
grid.arrange(survtitle1,survtitle2, ncol=2)
```

It can be seen that men with cheaper tickets have lower survival rates in comparison to the other titles on the ship. Young passengers, female passengers and nobles seem to have a higher survival rate. Next graph below shows the average fare per ticket type whereas the one on the right shows the number of observations for each ticket type:

```{r visual check with available data3, cache=TRUE, echo=FALSE, fig.height = 2.5}
ticket_avFare <- dat.full.train %>% group_by(Ticket) %>% summarise(n=n(),avFare = mean(Fare, na.rm = T))
t1 <- g + geom_col(data = ticket_avFare,aes(x=Ticket,y=avFare),fill = "deepskyblue3")
t2 <- g + geom_col(data = ticket_avFare,aes(x=Ticket,y=n),fill = "salmon3")
grid.arrange(t1,t2, ncol=2)
```

It can be seen that the average ticket prices are similar between types 3 and 7 and between types 4 and 6. Ticket type 5 is highest compared to other ticket types. On the other hand, types 3 and 7 are owned by lowest number of passengers on the ship whereas type 6 is owned most of the passengers. As mentioned before, the visual exploratory checks are performed only on the training set. However in the summary table above, there was also a ticket type 2 which is coming from the test set. It will be hard to make a prediction about this ticket type if we can't model it on the training set. There are 3 passengers that have ticket type 2, 14 passengers with ticket type 3 and 46 passengers with ticket type 7. Based on the assumption that the average fee paid for these ticket types are similar, it might make sense to combine these three ticket types before modeling. The next two graphs below show the relationship between these ticket types and the survival rate. The one on the left shows the total number of counts represented by coloured bars with green representing the number of survived passengers and red representing the ones that haven't survived the accident. The normalized version showing the proportion of survived and not survived passengers for each ticket type is shown on the below right graph:

```{r visual check with available data4, cache=TRUE, echo=FALSE, fig.height = 2.5}
survTicket1 <- g + geom_bar(data = dat.full.train, aes(x=Ticket, fill = Survived), position = "stack")
survTicket2 <- g + geom_bar(data = dat.full.train, aes(x=Ticket, fill = Survived), position = "fill")
grid.arrange(survTicket1,survTicket2, ncol=2)
```

It is a bit hard to make separate conclusions for each ticket type but an obvious one is that the passengers having expensive ticket types have higher survival rates. Next graph below shows the average fare per passenger class whereas the one on the right shows the number of observations for each passenger class:

```{r visual check with available data5, cache=TRUE, echo=FALSE, fig.height = 2.5}
Pclass_avFare <- dat.full.train %>% group_by(Pclass) %>% summarise(n=n(),avFare = mean(Fare, na.rm = T))
pc1 <- g + geom_col(data = Pclass_avFare,aes(x=Pclass,y=avFare),fill = "deepskyblue3")
pc2 <- g + geom_col(data = Pclass_avFare,aes(x=Pclass,y=n),fill = "salmon3")
grid.arrange(pc1,pc2, ncol=2)
```

As expected, class 1 passengers pay the highest price compared to other passenger classes. It can also be observed that Class 3 passengers have the highest number. The interesting observation is that class 2 passengers are less in amount in comparison to class 1 passengers. The next two graphs below show the relationship between these passenger classes and the survival rate. The one on the left shows the total number of counts represented by coloured bars with green representing the number of survived passengers and red representing the ones that haven't survived the accident. The normalized version showing the proportion of survived and not survived passengers for each passenger class is shown on the below right graph:   
```{r visual check with available data6, cache=TRUE, echo=FALSE, fig.height = 2.5}
survClass1 <- g + geom_bar(data = dat.full.train, aes(x=Pclass, fill = Survived), position = "stack")
survClass2 <- g + geom_bar(data = dat.full.train, aes(x=Pclass, fill = Survived), position = "fill")
grid.arrange(survClass1,survClass2, ncol=2)
```

The survival rate is proportional to the passenger class with passengers that pay the highest price survive the most proportionally. Next we will check the cabin predictor in the same manner.

```{r visual check with available data7, cache=TRUE, echo=FALSE, fig.height = 2.5}
cabin_avFare <- dat.full.train %>% group_by(Cabin) %>% summarise(n=n(),avFare = mean(Fare, na.rm = T))
cab1 <- g + geom_col(data = cabin_avFare,aes(x=Cabin,y=avFare),fill = "deepskyblue3")
cab2 <- g + geom_col(data = cabin_avFare,aes(x=Cabin,y=n),fill = "salmon3")
grid.arrange(cab1,cab2, ncol=2)
```

As mentioned before, the number of missing values for the cabin predictor is very high. This means either the cabin predictor will be dropped from analysis, which will result in loss of information, or a logic will be applied for imputation of the missing values. The following graphs show the relationship of the cabin predictor to survival rate:

```{r visual check with available data8, cache=TRUE, echo=FALSE, fig.height = 2.5}
survCabin1 <- g + geom_bar(data = dat.full.train, aes(x=Cabin, fill = Survived), position = "stack")
survCabin2 <- g + geom_bar(data = dat.full.train, aes(x=Cabin, fill = Survived), position = "fill")
grid.arrange(survCabin1,survCabin2, ncol=2)
```

There is not a particular pattern that can be noticed immediately but again cabins with passengers that have paid higher ticket fees tend to have higher survival rate. Next we will check the family size predictor in similar fashion to other predictors above:

```{r visual check with available data9, cache=TRUE, echo=FALSE, fig.height = 2.5}
fSize_avFare <- dat.full.train %>% group_by(fSize) %>% summarise(n=n(),avFare = mean(Fare, na.rm = T))
f1 <- g + geom_col(data = fSize_avFare,aes(x=fSize,y=avFare),fill = "deepskyblue3")
f2 <- g + geom_col(data = fSize_avFare,aes(x=fSize,y=n),fill = "salmon3")
grid.arrange(f1,f2, ncol=2)
```

Large families have higher average fare but less in total number. The highest number of individuals are single ones whereas small families also make up a big part of the total number of passengers. The next two graphs show the family size relationship to survival rate:

```{r visual check with available data10, cache=TRUE, echo=FALSE, fig.height = 2.5}
survfSize1 <- g + geom_bar(data = dat.full.train, aes(x=fSize, fill = Survived), position = "stack")
survfSize2 <- g + geom_bar(data = dat.full.train, aes(x=fSize, fill = Survived), position = "fill")
grid.arrange(survfSize1,survfSize2, ncol=2)
```

It can be seen that large families and single passengers don't have high survival rate whereas small families have a higher survival rate. Still, the total number of survived single passengers and small families are very close to each other. The next predictor that is going to be observed is the embarkation port:

```{r visual check with available data11, cache=TRUE, echo=FALSE, fig.height = 2.5}
embarked_avFare <- dat.full.train %>% group_by(Embarked) %>% summarise(n=n(),avFare = mean(Fare, na.rm = T))
em1 <- g + geom_col(data = embarked_avFare,aes(x=Embarked,y=avFare),fill = "deepskyblue3")
em2 <- g + geom_col(data = embarked_avFare,aes(x=Embarked,y=n),fill = "salmon3")
grid.arrange(em1,em2, ncol=2)
```

The figure on the upper left shows the average fare for passengers that embarked from different ports. The 2 missing values have the highest rate. The passengers that embarked from port S are the highest in number. Next, the relationship to survival rate will be checked:

```{r visual check with available data12, cache=TRUE, echo=FALSE, fig.height = 2.5}
survEmbarked1 <- g + geom_bar(data = dat.full.train, aes(x=Embarked, fill = Survived), position = "stack")
survEmbarked2 <- g + geom_bar(data = dat.full.train, aes(x=Embarked, fill = Survived), position = "fill")
grid.arrange(survEmbarked1,survEmbarked2, ncol=2)
```

It can again be seen that passengers that have paid the highest rate for their tickets in average embarked from a similar port and have a higher survival rate. We will check the sex variable below in similar fashion although it doesn't totally make sense to check the ticket fee for each sex. We will do it for the sake of keeping the same format but will add more specific plots for the predictors:

```{r visual check with available data13, cache=TRUE, echo=FALSE, fig.height = 2.5}
sex_avFare <- dat.full.train %>% group_by(isMale) %>% summarise(n=n(),avFare = mean(Fare, na.rm = T))
sex1 <- g + geom_col(data = sex_avFare,aes(x=isMale,y=avFare),fill = "deepskyblue3")
sex2 <- g + geom_col(data = sex_avFare,aes(x=isMale,y=n),fill = "salmon3")
grid.arrange(sex1,sex2, ncol=2)
```

Interestingly, it looks like female passengers have paid more in average for their tickets. There are more male passengers on board which might show the indication that the male passengers are part of a lower passenger class. We will check the relationship to the survival rate now:

```{r visual check with available data14, cache=TRUE, echo=FALSE, fig.height = 2.5}
survSex1 <- g + geom_bar(data = dat.full.train, aes(x=isMale, fill = Survived), position = "stack")
survSex2 <- g + geom_bar(data = dat.full.train, aes(x=isMale, fill = Survived), position = "fill")
grid.arrange(survSex1,survSex2, ncol=2)
```

It can be easily noticed that the survival rate is high among female passengers. This predictor is highly correlated to the suvival rate.

We've checked all the categorical variables in relation to the average fee and survival rate. However, it doesn't actually make total sense to check all of the categorical variables in relation to average ticket fee such as cabin type or titles. The following graphs will try to add more meaningful visual checks for the available data.

First we will check the relationship between the cabin and the family size:

```{r visual check with available data15, cache=TRUE, echo=FALSE, fig.height = 2.5}
cabFsize1 <- g + geom_bar(data = dat.full.train, aes(x=Cabin, fill = fSize), position = "stack")
cabFsize2 <- g + geom_bar(data = dat.full.train, aes(x=Cabin, fill = fSize), position = "fill")
grid.arrange(cabFsize1,cabFsize2, ncol=2)
```

The number of missing values are so high that the graph on the left above doesn't give much information. Data imputation is necessary to be able to get something meaningful out of the above plot. We will get back to this plot after the imputation is finalized in the next section. We can now check the average age of each title to be able to assess if it makes sense to impute the missing age observations based on the title:

```{r visual check with available data16, cache=TRUE, echo=FALSE, fig.height = 2.5, message=FALSE}
title_avAge <- dat.full.train %>% group_by(title) %>% summarise(n=n(),avAge = mean(Age, na.rm = T))
g + geom_col(data = title_avAge,aes(x=title,y=avAge),fill = "deepskyblue3")
```

It can be seen that there is correlation between the average age and the title so it makes sense to use title to impute the missing age values. However, before that let's have a look into the continous predictor fare for tickets.

```{r visual check with available data17, cache=TRUE, echo=FALSE, fig.height = 2.5, message=FALSE}
fare1 <- g + geom_histogram(data = dat.full.train, aes(Fare), color = "black", fill = "deepskyblue3")
fare2 <- g + geom_boxplot(data = dat.full.train, aes(Survived, Fare), color = "black", fill = "salmon3")
grid.arrange(fare1,fare2, ncol=2)
```

The fare predictor has a skewed distribution so a log transformation or normalization might be necessary. Age predictor has a lot of missing observations so we will look at the available data for now in the same manner as the fare predictor:

```{r visual check with available data18, cache=TRUE, echo=FALSE, fig.height = 2.5, message=FALSE, warning=FALSE}
age1 <- g + geom_histogram(data = dat.full.train, aes(Age), color = "black", fill = "deepskyblue3")
age2 <- g + geom_boxplot(data = dat.full.train, aes(Survived, Age), color = "black", fill = "salmon3")
grid.arrange(age1,age2, ncol=2)
```

We can see a slight implication on the above right figure of younger passengers having a higher survival rate in general. In the next sub-section, we will try to impute the missing observations, modify the data to its final form before modelling and show some final visual exploratory checks.

### 2.1 Data Imputation

In the summary table above for a combined dataset of both training and test datasets,  we have observed 263 missing observations for Age predictor, 1 for Fare predictor, 1014 for Cabin predictor and 2 for Embarked predictor. We will try to impute all the missing observations although an evaluation needs to be made for the Cabin predictor if the imputed dataset makes sense. First let's have a look at a graphical representation of the missing observations:

```{r imputation 1, cache=TRUE, echo=FALSE, fig.height = 2.5, message=FALSE, warning=FALSE}
aggr(dat.full, numbers=TRUE, prop = c(TRUE,FALSE))
```

The plot on the left shows the percentage of the missing data per predictor. It can be seen as mentioned before as well that Age and Cabin predictors have the highest number of missing observations. The plot on the right shows the missing observations as a combination. Each row can be read as a specific combination of observations with columns representing the predictors. For example the first row represents the single observation where both Fare and Cabin predictors are missing at the same time. The second row represents the two observations where missing values are for the Embarked predictor. The same goes for all the combinations of missing observations with the fifth row representing 270 observations without any missing values. The methodology for imputation of each predictor will be explained in the following paragraphs. The imputation process will be carried out for both training and test sets combined.

We will start imputing the missing observations in the Age predictor. We wil replace the missing observations using mice package which is implementing multiple imputation methods for missing observations. The details of the algorithm will not be discussed here but they can be accessed via the package official document.  The Age predictor will be imputed by using title and family size predictors as they are assessed to be the most suitable ones for predicting the missing Age observations. The method implemented will be weighted predictive mean matching and 20 multiple imputations will be performed.

```{r imputation 2, cache=TRUE, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
imp_Age <- mice(data = data.frame(Age=dat.full$Age, title=dat.full$title, fSize = dat.full$fSize), m=20, printFlag = FALSE, method = "midastouch")
imputed_Age <- complete(imp_Age)
dat.full$Age <- as.numeric(imputed_Age[,1])
```

The plot below shows the density plot for the Age predictor with blue curve representing the observed data and the red curves representing the imputed data for each imputation:

```{r imputation 3, cache=TRUE, echo=FALSE, fig.height = 2.5, message=FALSE, warning=FALSE}
densityplot(imp_Age)
```

It can be seen from the above plot that the imputed data is very close to the existing data. Now we can have a look at the exploratory graphs for the Age predictor after imputation for all the missing observations is completed. The visualization will represent only the training set values as done before:

```{r imputation 4, cache=TRUE, echo=FALSE, fig.height = 2.5, message=FALSE, warning=FALSE}
dat.full.train_temp <- data.frame(Survived = as.factor(dat.train[,2]),dat.full[1:891,])
age21 <- g + geom_histogram(data = dat.full.train_temp, aes(Age), color = "black", fill = "deepskyblue3")
age22 <- g + geom_boxplot(data = dat.full.train_temp, aes(Survived, Age), color = "black", fill = "salmon3")
grid.arrange(age21,age22, ncol=2)
```

It can be observed above that the number of missing observations are mostly around 30 years of age. Nevertheless, the general shape is very similar to the raw data.

Next we will look at the imputation of 2 missing observations from Embarked predictor. As the number of missing observations is very low compared to the total number of observations and as the missing observations have both survived the accident, they will be imputed as part of the factor level, which is "C". 

The other missing observation is from Fare predictor and it will be imputed by replacing the missing observation with the median value of the existing observations. Again, this imputation isn't expected to have a huge effect on the analysis.

```{r imputation 5, cache=TRUE, eval=TRUE, echo=FALSE}
dat.full$Embarked <- as.character(dat.full$Embarked)
for (i in 1:length(dat.full$Embarked)){
  if(is.na(dat.full$Embarked[i])){
    dat.full$Embarked[i] <- "C"  
  } 
}
dat.full$Embarked <- as.factor(dat.full$Embarked)


for (i in 1:length(dat.full$Fare)){
  if(is.na(dat.full$Fare[i])){
    dat.full$Fare[i] <- as.numeric(median(dat.full.train_temp$Fare, na.rm = T))  
  } 
}
```

The final predictor to be imputed is Cabin. It has the most amount of missing observations so it is expected to add noise to the analysis. Again mice package will be used for imputation using all the available and previously imputed data with the method of polytomous logistic regression for prediction of missing values.

```{r imputation 6, cache=TRUE, eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
imp_Cabin <- mice(data = dat.full, m=20, printFlag = FALSE)
imputed_Cabin <- complete(imp_Cabin)
dat.full <- imputed_Cabin
```

Again the density plot for the observed data and the imputed data can be seen in the below graph:

```{r imputation 7, cache=TRUE, echo=FALSE, fig.height = 2.5, message=FALSE, warning=FALSE}
densityplot(imp_Cabin,  ~ Cabin)
```

The x-axis numbers actually represent the levels of the Cabin predictor starting from "A". The imputed data shows a different pattern than the available data which can be expected due to high number of missing observations in comparison to the existing ones. It can be seen in the above plot that the imputed values are gathered around level "6" which represents the cabin type "F". Now it makes sense to look at the exploratory graphs one more time:

```{r imputation 8,cache=TRUE, echo=FALSE, fig.height=2.5, message=FALSE, warning=FALSE}
dat.full.train <- data.frame(Survived = as.factor(dat.train[,2]),dat.full[1:891,])
cabin_avFare <- dat.full.train %>% group_by(Cabin) %>% summarise(n=n(),avFare = mean(Fare, na.rm = T))
cab21 <- g + geom_col(data = cabin_avFare,aes(x=Cabin,y=avFare),fill = "deepskyblue3")
cab22 <- g + geom_col(data = cabin_avFare,aes(x=Cabin,y=n),fill = "salmon3")
grid.arrange(cab21,cab22, ncol=2)
```

It can be seen from the graph above that cabin types "B" and "C" have the passengers with the highest average fare whereas the number of passengers are highest in cabin types "F" and "G". The following graphs show the relationship of the cabin predictor to survival rate:

```{r imputation 9, cache=TRUE, echo=FALSE, fig.height = 2.5, message=FALSE, warning=FALSE}
survCabin21 <- g + geom_bar(data = dat.full.train, aes(x=Cabin, fill = Survived), position = "stack")
survCabin22 <- g + geom_bar(data = dat.full.train, aes(x=Cabin, fill = Survived), position = "fill")
grid.arrange(survCabin21,survCabin22, ncol=2)
```

It can be observed that the correlation between the passengers that reside in cabins with highest ticket fee tend to have higher survival rates. We can also duplicate the plot where the relationship between cabin and family size can be seen after the imputations are completed:

```{r imputation 10, cache=TRUE, echo=FALSE, fig.height = 2.5, message=FALSE, warning=FALSE}
cabFsize21 <- g + geom_bar(data = dat.full.train, aes(x=Cabin, fill = fSize), position = "stack")
cabFsize22 <- g + geom_bar(data = dat.full.train, aes(x=Cabin, fill = fSize), position = "fill")
grid.arrange(cabFsize21,cabFsize22, ncol=2)
```

As expected, cabins for larger families tend to have lower survival rate whereas the ones for small families tend to have higher survival rate. Next, we will look into final transformations of predictors before we start the modelling.

### 2.2 Data Transformation

In this section, we will look into transforming the imputed data to its final shape. We will start with the title predictor. It was observed that "Noble" and "Soldier" titles had few observations compared to other factor levels and therefore could end up in unbalanced groups if hierarchical modeling is going to be applied. The number of observations for each title group can be seen below. The number of individuals are presented for both training and test data sets combined:

```{r final transformation 1, cache=TRUE, eval=TRUE, echo=FALSE}
summary(dat.full$title)
```

The "Noble" title group will be combined with the "Master" title group and will be named "Other". The "Soldier" title group will be combined with the "Mr" title group. These decisions are made based on the similarity of the relationship to the survival rate.

```{r final transformation 2, cache=TRUE, eval=TRUE, echo=FALSE}
# Making title more balanced
dat.full$title <- as.character(dat.full$title)
for (i in 1:length(dat.full$title)){
  if(grepl("Master|Noble",dat.full$title[i]) == T){
    dat.full$title[i] <- "Other"  
  } 
  if(grepl("Soldier",dat.full$title[i]) == T){
    dat.full$title[i] <- "Mr"  
  } 
}
dat.full$title <- as.factor(dat.full$title)
dat.full$title <- droplevels(dat.full$title)
```

The title factor levels have changed as below after the transformation:

```{r final transformation 3, cache=TRUE, eval=TRUE, echo=FALSE}
summary(dat.full$title)
```

Next we we will look into the ticket predictor. As mentioned in the initial part of the exploratory analysis, the test set had a ticket type "2" that the training set didn't have. The number of type "2" tickets are very low like in types "3" and "7" with similar fares. We can have a look at the number of observations for each level for both training and test datasets below:

```{r final transformation 4, cache=TRUE, eval=TRUE, echo=FALSE}
summary(dat.full$Ticket)
```

The transformation here would be to combine types "2", "3" and "7" as "Other" ticket type so that the modeling can be performed with the training set and proper predictions can be applied to the test set. The final number of observations for each ticket type can be seen below after the transformation is applied:

```{r final transformation 5, cache=TRUE, eval=TRUE, echo=FALSE}
dat.full$Ticket <- as.character(dat.full$Ticket)
for (i in 1:length(dat.full$Ticket)){
  if(grepl("2|3|7",dat.full$Ticket[i]) == T){
    dat.full$Ticket[i] <- "Other"  
  }
}
dat.full$Ticket <- as.factor(dat.full$Ticket)
dat.full$Ticket <- droplevels(dat.full$Ticket)

summary(dat.full$Ticket)
```

The final transformation will be applied to the Fare predictor. As has been observed, this predictor has a skewed distribution and it will be beneficial to use the transformed version. The squareroot will be used for transforming the positive skew of Fare predictor. The distribution for the transformed version of this predictor for only training set can be seen below:

```{r final transformation 6, cache=TRUE, eval=TRUE, echo=FALSE, message=FALSE, fig.height = 2.5}
# Sqrt transform continuous  fare variable
dat.full$FareSqrt <- sqrt(dat.full$Fare)
dat.full <- dat.full[ , -which(names(dat.full) %in% c("Fare"))]

dat.full.train <- dat.full[1:891,]

g + geom_histogram(data = dat.full.train, aes(FareSqrt), color = "black", fill = "deepskyblue3")

train_imp <- data.frame(PassengerId = dat.full[1:891,1], Survived = as.factor(dat.train[,2]), dat.full[1:891,2:10])
test_imp <- dat.full[892:1309,]
```

It can be observed that the transformation wasn't enough to get rid of the whole skewness. The summary of the training dataset can be seen below:

```{r final transformation 7, cache=TRUE, eval=TRUE, echo=FALSE}
summary(train_imp)
```

Both the training and the test datasets have been nomalized to be used for analysis and prediction. The summary of the normalized version of the training set can be seen below:

```{r model input, cache=TRUE, eval=TRUE, echo=FALSE, message=FALSE}
y.train <- as.numeric(train_imp$Survived) # outcome
y.train <- y.train-1
pId.train <- train_imp$PassengerId # ID for predictions
pId.test <- test_imp$PassengerId # ID for predictions

predictor.variables <- colnames(train_imp)[3:length(colnames(train_imp))]
predictor1.variables <- predictor.variables[predictor.variables != "title"]
predictor2.variables <- predictor.variables[predictor.variables != "Pclass"]
predictor3.variables <- predictor.variables[predictor.variables != "Cabin"]
predictor4.variables <- predictor.variables[!predictor.variables %in% c("Ticket","Embarked")]
predictor5.variables <- predictor.variables[!predictor.variables %in% c("Pclass","isMale")]
predictor6.variables <- predictor.variables[!predictor.variables %in% c("Pclass","title")]
predictor7.variables <- predictor.variables[!predictor.variables %in% c("Pclass","Cabin")]

formula_train <- as.formula(y.train~ .)
formula_test <- as.formula(~ .)

X <- subset(train_imp, select=predictor.variables)
X0.train <- model.matrix(formula_train, data = X)[,-1]
X <- subset(test_imp, select=predictor.variables)
X0.test <- model.matrix(formula_test, data = X)[,-1]

X <- subset(train_imp, select=predictor1.variables)
X1.train <- model.matrix(formula_train, data = X)[,-1]
X <- subset(test_imp, select=predictor1.variables)
X1.test <- model.matrix(formula_test, data = X)[,-1]

X <- subset(train_imp, select=predictor2.variables)
X2.train <- model.matrix(formula_train, data = X)[,-1]
X <- subset(test_imp, select=predictor2.variables)
X2.test <- model.matrix(formula_test, data = X)[,-1]

X <- subset(train_imp, select=predictor3.variables)
X3.train <- model.matrix(formula_train, data = X)[,-1]
X <- subset(test_imp, select=predictor3.variables)
X3.test <- model.matrix(formula_test, data = X)[,-1]

X <- subset(train_imp, select=predictor4.variables)
X4.train <- model.matrix(formula_train, data = X)[,-1]
X <- subset(test_imp, select=predictor4.variables)
X4.test <- model.matrix(formula_test, data = X)[,-1]

X <- subset(train_imp, select=predictor5.variables)
X5.train <- model.matrix(formula_train, data = X)[,-1]
X <- subset(test_imp, select=predictor5.variables)
X5.test <- model.matrix(formula_test, data = X)[,-1]

X <- subset(train_imp, select=predictor6.variables)
X6.train <- model.matrix(formula_train, data = X)[,-1]
X <- subset(test_imp, select=predictor6.variables)
X6.test <- model.matrix(formula_test, data = X)[,-1]

X <- subset(train_imp, select=predictor7.variables)
X7.train <- model.matrix(formula_train, data = X)[,-1]
X <- subset(test_imp, select=predictor7.variables)
X7.test <- model.matrix(formula_test, data = X)[,-1]

colnames(X0.train) <- gsub(" ", "", colnames(X0.train), fixed = TRUE)
colnames(X0.test) <- gsub(" ", "", colnames(X0.test), fixed = TRUE)
colnames(X1.train) <- gsub(" ", "", colnames(X1.train), fixed = TRUE)
colnames(X1.test) <- gsub(" ", "", colnames(X1.test), fixed = TRUE)
colnames(X2.train) <- gsub(" ", "", colnames(X2.train), fixed = TRUE)
colnames(X2.test) <- gsub(" ", "", colnames(X2.test), fixed = TRUE)
colnames(X3.train) <- gsub(" ", "", colnames(X3.train), fixed = TRUE)
colnames(X3.test) <- gsub(" ", "", colnames(X3.test), fixed = TRUE)
colnames(X4.train) <- gsub(" ", "", colnames(X4.train), fixed = TRUE)
colnames(X4.test) <- gsub(" ", "", colnames(X4.test), fixed = TRUE)
colnames(X5.train) <- gsub(" ", "", colnames(X5.train), fixed = TRUE)
colnames(X5.test) <- gsub(" ", "", colnames(X5.test), fixed = TRUE)
colnames(X6.train) <- gsub(" ", "", colnames(X6.train), fixed = TRUE)
colnames(X6.test) <- gsub(" ", "", colnames(X6.test), fixed = TRUE)
colnames(X7.train) <- gsub(" ", "", colnames(X7.train), fixed = TRUE)
colnames(X7.test) <- gsub(" ", "", colnames(X7.test), fixed = TRUE)

X0.train <- scale(X0.train)
X0.test <- scale(X0.test)
X1.train <- scale(X1.train)
X1.test <- scale(X1.test)
X2.train <- scale(X2.train)
X2.test <- scale(X2.test)
X3.train <- scale(X3.train)
X3.test <- scale(X3.test)
X4.train <- scale(X4.train)
X4.test <- scale(X4.test)
X5.train <- scale(X5.train)
X5.test <- scale(X5.test)
X6.train <- scale(X6.train)
X6.test <- scale(X6.test)
X7.train <- scale(X7.train)
X7.test <- scale(X7.test)

titleTrain <- as.numeric(train_imp$title)
titleTest <- as.numeric(test_imp$title)

pClassTrain <- as.numeric(train_imp$Pclass)
pClassTest <- as.numeric(test_imp$Pclass)

ticketTrain <- as.numeric(train_imp$Ticket)
ticketTest <- as.numeric(test_imp$Ticket)

embarkedTrain <- as.numeric(train_imp$Embarked)
embarkedTest <- as.numeric(test_imp$Embarked)

isMaleTrain <- as.numeric(train_imp$isMale)
isMaleTest <- as.numeric(test_imp$isMale)

cabinTrain <- as.numeric(train_imp$Cabin)
cabinTest <- as.numeric(test_imp$Cabin)

summary(X0.train)
```

Now, both the training and test datasets are ready for prior predictive checking.

## 3- Prior Predictive Checking

In this section, we will perform a prior predictive check by only using the priors and no data.The reason for making prior predictive analysis is to make a sanity check on the priors without using the likelihood. 

In our analysis, we will look into different models where we will need to move some of the predictors as a grouping variable and the rest for estimating the coefficients. Such an example can be a hierarchical model where we might need to use the title predictor for grouping the rest of the variables according to their title. In any case, we will use weakly informative robust prior of student_t(7,0,2.5) for both population-level and group-level parameter means with an lkj(2) distribution for the correlation matrix of the group level effects while estimating the mean via a multivariate normal distribution. Below we will demonstrate the prior predictive distribution for the non-hierarchical model including all the predictors:

```{r prior predictive checking, cache=TRUE, eval=TRUE, echo=FALSE, fig.height = 2.5, message=FALSE}
# Prior predictive checks
df0.train <- as.data.frame(cbind(y.train, X0.train))

title <- titleTrain
df1.train <- as.data.frame(cbind(y.train, X1.train,title))

pClass <- pClassTrain
df2.train <- as.data.frame(cbind(y.train, X2.train,pClass))

Cabin <- cabinTrain
df3.train <- as.data.frame(cbind(y.train, X3.train, Cabin))

ticket <- ticketTrain
embarked <- embarkedTrain
df4.train <- as.data.frame(cbind(y.train, X4.train,ticket,embarked))

isMale <- isMaleTrain
df5.train <- as.data.frame(cbind(y.train,X5.train,pClass,isMale))

df6.train <- as.data.frame(cbind(y.train,X6.train,pClass,title))

df7.train <- as.data.frame(cbind(y.train,X7.train,pClass,Cabin))

formula_only_intercept_population <- as.formula(y.train ~ 1)
formula_only_intercept_population <- bf(formula = formula_only_intercept_population, center = F)

formula_only_intercept_population_and_group <- as.formula(y.train ~ 1 + (1|title))
formula_only_intercept_population_and_group <- bf(formula = formula_only_intercept_population_and_group, center = F)

formula_all_population <- as.formula(paste("y.train ~", paste0("1+",paste(colnames(df0.train)[2:length(colnames(df0.train))], collapse = "+"))))
formula_all_population <- bf(formula = formula_all_population, center = F)

formula_hier1_title <- as.formula(paste("y.train ~", paste0("1+",paste(colnames(df1.train)[2:(length(colnames(df1.train))-1)], collapse = "+")),"+(1+",paste(colnames(df1.train)[2:(length(colnames(df1.train))-1)], collapse = "+"),"|title)"))
formula_hier1_title <- bf(formula = formula_hier1_title, center = F)

formula_hier2_pClass <- as.formula(paste("y.train ~", paste0("1+",paste(colnames(df2.train)[2:(length(colnames(df2.train))-1)], collapse = "+")),"+(1+",paste(colnames(df2.train)[2:(length(colnames(df2.train))-1)], collapse = "+"),"|pClass)"))
formula_hier2_pClass <- bf(formula = formula_hier2_pClass, center = F)

prior0 <- prior(student_t(7,0,2.5), class = b)
prior1 <- prior(student_t(7,0,2.5), class = b) + prior(lkj(2), class = cor)
prior2 <- prior(horseshoe(1), class = b)

all_predictors.fit <- brm(formula = formula_all_population,
                                       data = df0.train,
                                       prior = prior0,
                                       sample_prior = "only",
                                       family = bernoulli(link="logit"),
                                       cores = parallel::detectCores(),
                                       control = list(adapt_delta = 0.98, max_treedepth = 15))

yrep.prior0 <- posterior_predict(all_predictors.fit)

# Graphical checks
pp_check(y.train, yrep.prior0, ppc_dens_overlay)
```

In the above figure, the solid black line is the actual distribution of the training set represented by "y" and the blue lines are the simulations with parameters picked from a robust student-t prior distribution represented by "y-rep" for the population-level effects. A bernoulli model with a logit link function has been used for modelling. By looking at the figure, it can be concluded that the chosen prior is suitable for analysis and prediction using the same model.

## 4 - Model Fitting and Algorithm Diagnostics
```{r model fitting, cache=TRUE, eval=FALSE, echo=FALSE}
df0.test <- as.data.frame(cbind(X0.test))

title <- titleTest
df1.test <- as.data.frame(cbind(X1.test,title))

pClass <- pClassTest
df2.test <- as.data.frame(cbind(X2.test,pClass))

# model 1: Only intercept model at population level----------------------------------------------------
only_population_intercept.fit <- brm(formula = formula_only_intercept_population,
                                      data = df1.train,
                                      prior = prior0,
                                      family = bernoulli(link="logit"),
                                      cores = parallel::detectCores(),
                                      control = list(adapt_delta = 0.98, max_treedepth = 15),
                                      save_pars = save_pars(all = TRUE),
                                      seed = 187)

# model 2: All predictors model with population effects only------------------------------------------
population_all.fit <- brm(formula = formula_all_population,
                                      data = df0.train,
                                      prior = prior0,
                                      family = bernoulli(link="logit"),
                                      cores = parallel::detectCores(),
                                      control = list(adapt_delta = 0.98, max_treedepth = 15),
                                      save_pars = save_pars(all = TRUE),
                                      seed = 187)

# model 3: All predictors model with population effects only and horseshoe prior----------------------
population_all_hs.fit <- brm(formula = formula_all_population,
                                      data = df0.train,
                                      prior = prior2,
                                      family = bernoulli(link="logit"),
                                      cores = parallel::detectCores(),
                                      control = list(adapt_delta = 0.98, max_treedepth = 15),
                                      save_pars = save_pars(all = TRUE),
                                      seed = 187)

# model 4: Only intercept model at population level and grouping level by title------------------------
population_and_group_intercept.fit <- brm(formula = formula_only_intercept_population_and_group,
                                      data = df1.train,
                                      prior = prior0,
                                      family = bernoulli(link="logit"),
                                      cores = parallel::detectCores(),
                                      control = list(adapt_delta = 0.98, max_treedepth = 15),
                                      save_pars = save_pars(all = TRUE),
                                      seed = 187)

# model 5: All predictors model with population and group effects by title----------------------------
multi_hier1_title_post.fit <- brm(formula = formula_hier1_title,
                                       data = df1.train,
                                       prior = prior1,
                                       family = bernoulli(link="logit"),
                                       cores = parallel::detectCores(),
                                       control = list(adapt_delta = 0.98, max_treedepth = 15),
                                       save_pars = save_pars(all = TRUE),
                                       seed = 187)

# model 6: All predictors model with population and group effects by pClass----------------------------
multi_hier2_pClass_post.fit <- brm(formula = formula_hier2_pClass,
                                       data = df2.train,
                                       prior = prior1,
                                       family = bernoulli(link="logit"),
                                       cores = parallel::detectCores(),
                                       control = list(adapt_delta = 0.98, max_treedepth = 15),
                                       save_pars = save_pars(all = TRUE),
                                       seed = 187)
```


```{r simulating posterior predictions, cache=TRUE, eval=FALSE, echo=FALSE}
# making predictions
y_pred1 <- posterior_predict(only_population_intercept.fit, df0.test)
y_pred2 <- posterior_predict(population_all.fit, df0.test)
y_pred3 <- posterior_predict(population_all_hs.fit, df0.test)
y_pred4 <- posterior_predict(population_and_group_intercept.fit, df1.test)
y_pred5 <- posterior_predict(multi_hier1_title_post.fit, df1.test)
y_pred6 <- posterior_predict(multi_hier2_pClass_post.fit, df2.test)

yrep1.posterior <- posterior_predict(only_population_intercept.fit)
yrep2.posterior <- posterior_predict(population_all.fit)
yrep3.posterior <- posterior_predict(population_all_hs.fit)
yrep4.posterior <- posterior_predict(population_and_group_intercept.fit)
yrep5.posterior <- posterior_predict(multi_hier1_title_post.fit)
yrep6.posterior <- posterior_predict(multi_hier2_pClass_post.fit)
```

## 5 - Posterior Predictive Checking
```{r posterior predictive checking, cache=TRUE, eval=FALSE, echo=FALSE}
summary(only_population_intercept.fit)
summary(population_all.fit)
summary(population_all_hs.fit)
summary(population_and_group_intercept.fit)
summary(multi_hier1_title_post.fit)
summary(multi_hier2_pClass_post.fit)

plot(conditional_effects(population_all.fit), ask = F)
plot(conditional_effects(multi_hier1_title_post.fit), ask = F)

pp_check(y.train, yrep1.posterior[1:50,], ppc_dens_overlay)
pp_check(y.train, yrep2.posterior[1:50,], ppc_dens_overlay)
pp_check(y.train, yrep3.posterior[1:50,], ppc_dens_overlay)
pp_check(y.train, yrep4.posterior[1:50,], ppc_dens_overlay)
pp_check(y.train, yrep5.posterior[1:50,], ppc_dens_overlay)
pp_check(y.train, yrep6.posterior[1:50,], ppc_dens_overlay)

pp_check(y.train, yrep5.posterior, fun = "stat_grouped", group = train_imp$title, stat = "mean")
pp_check(y.train, yrep6.posterior, fun = "stat_grouped", group = train_imp$pClass, stat = "mean")

mcmc_areas(population_all_hs.fit, pars = c("b_Intercept",paste0("b_",colnames(df1.train)[2:(length(colnames(df1.train))-1)])))

mcmc_hist(multi_hier1_title_post.fit, pars = c("b_Intercept",paste0("b_",colnames(df1.train)[2:(length(colnames(df1.train))-1)])))

# posterior classification accuracy for model5
pred5 <- colMeans(yrep5.posterior)
pr5 <- as.integer(pred5 >= 0.5)
model5_pca <- round(mean(xor(pr5,as.integer(y.train==0))),2)

# posterior balanced classification accuracy for model5
model5_bpca <- round((mean(xor(pr5[y.train==0]>0.5,as.integer(y.train[y.train==0])))+mean(xor(pr5[y.train==1]<0.5,as.integer(y.train[y.train==1]))))/2,2)
```

## 6 - Additional Models and Model Improvements
```{r additional models, cache=TRUE, eval=FALSE, echo=FALSE}

# Interaction terms added to all predictors with population terms only model
formula_all_population_interactions <- as.formula(y.train ~ 1 + Pclass2 + Pclass3 + Ticket5 + Ticket6 + TicketC + TicketOther + TicketP + TicketS + EmbarkedQ + EmbarkedS + titleMr + titleMrs + titleOther + fSizesingleton + fSizesmall + isMale1 + FareLog + AgeLog + fSizesingleton:FareLog + Pclass3:AgeLog + FareLog:AgeLog + I(AgeLog^2))

formula_all_population_interactions <- bf(formula = formula_all_population_interactions, center = F)

population_all_interactions.fit <- brm(formula = formula_all_population_interactions,
                                      data = df0.train,
                                      prior = prior0,
                                      family = bernoulli(link="logit"),
                                      cores = parallel::detectCores(),
                                      control = list(adapt_delta = 0.98, max_treedepth = 15),
                                      save_pars = save_pars(all = TRUE),
                                      seed = 187)

# Interaction terms added to all predictors with hierarchical model grouped by title
formula_multi_hier1_title_interactions <- as.formula(y.train ~ 1 + Pclass2 + Pclass3 + Ticket5 + Ticket6 + TicketC + TicketOther + TicketP + TicketS + EmbarkedQ + EmbarkedS + fSizesingleton + fSizesmall + isMale1 + FareLog + AgeLog + fSizesingleton:FareLog + Pclass3:AgeLog + FareLog:AgeLog + I(AgeLog^2) + (1 + Pclass2 + Pclass3 + Ticket5 + Ticket6 + TicketC + TicketOther + TicketP + TicketS + EmbarkedQ + EmbarkedS + fSizesingleton + fSizesmall + isMale1 + FareLog + AgeLog | title))

formula_multi_hier1_title_interactions <- bf(formula = formula_multi_hier1_title_interactions, center = F)

multi_hier1_title_post_interactions.fit <- brm(formula = formula_multi_hier1_title_interactions,
                                       data = df1.train,
                                       prior = prior1,
                                       family = bernoulli(link="logit"),
                                       cores = parallel::detectCores(),
                                       control = list(adapt_delta = 0.98, max_treedepth = 15),
                                       save_pars = save_pars(all = TRUE),
                                       seed = 187)

# Interaction terms added to all predictors with hierarchical model grouped by pClass
formula_multi_hier2_pClass_interactions <- as.formula(y.train ~ 1 + Ticket5 + Ticket6 + TicketC + TicketOther + TicketP + TicketS + EmbarkedQ + EmbarkedS + titleMr + titleMrs + titleOther + fSizesingleton + fSizesmall + isMale1 + FareLog + AgeLog + fSizesingleton:FareLog + titleOther:AgeLog + FareLog:AgeLog + I(AgeLog^2) + (1 + Ticket5 + Ticket6 + TicketC + TicketOther + TicketP + TicketS + EmbarkedQ + EmbarkedS + titleMr + titleMrs + titleOther + fSizesingleton + fSizesmall + isMale1 + FareLog + AgeLog | pClass))

formula_multi_hier2_pClass_interactions <- bf(formula = formula_multi_hier2_pClass_interactions, center = F)

multi_hier2_pClass_post_interactions.fit <- brm(formula = formula_multi_hier2_pClass_interactions,
                                       data = df2.train,
                                       prior = prior1,
                                       family = bernoulli(link="logit"),
                                       cores = parallel::detectCores(),
                                       control = list(adapt_delta = 0.98, max_treedepth = 15),
                                       save_pars = save_pars(all = TRUE),
                                       seed = 187)

# Double grouping for hierarchical model with title and embarked
formula_hier3_doubleClass <- as.formula(paste("y.train ~", paste0("1+",paste(colnames(df3.train)[2:(length(colnames(df3.train))-2)], collapse = "+")),"+(1+",paste(colnames(df3.train)[2:(length(colnames(df3.train))-2)], collapse = "+"),"|(ticket:embarked))"))
formula_hier3_doubleClass <- bf(formula = formula_hier3_doubleClass, center = F)

multi_hier3_doubleClass.fit <- brm(formula = formula_hier3_doubleClass,
                                      data = df3.train,
                                      prior = prior1,
                                      family = bernoulli(link="logit"),
                                      cores = parallel::detectCores(),
                                      control = list(adapt_delta = 0.98, max_treedepth = 15),
                                      save_pars = save_pars(all = TRUE),
                                      seed = 187)

# Double grouping for hierarchical model with pClass and isMale
formula_hier4_doubleClass <- as.formula(paste("y.train ~", paste0("1+",paste(colnames(df4.train)[2:(length(colnames(df4.train))-2)], collapse = "+")),"+(1+",paste(colnames(df4.train)[2:(length(colnames(df4.train))-2)], collapse = "+"),"|(pClass:isMale))"))
formula_hier4_doubleClass <- bf(formula = formula_hier4_doubleClass, center = F)

multi_hier4_doubleClass.fit <- brm(formula = formula_hier4_doubleClass,
                                      data = df4.train,
                                      prior = prior1,
                                      family = bernoulli(link="logit"),
                                      cores = parallel::detectCores(),
                                      control = list(adapt_delta = 0.98, max_treedepth = 15),
                                      save_pars = save_pars(all = TRUE),
                                      seed = 187)

# Double grouping for hierarchical model with fare variable and grouped by pClass and title
formula_hier5_doubleClass <- as.formula(y.train ~ 1 + FareLog + (1 + FareLog | pClass:title))
formula_hier5_doubleClass <- bf(formula = formula_hier5_doubleClass, center = F)

multi_hier5_doubleClass.fit <- brm(formula = formula_hier5_doubleClass,
                                      data = df5.train,
                                      prior = prior1,
                                      family = bernoulli(link="logit"),
                                      cores = parallel::detectCores(),
                                      control = list(adapt_delta = 0.98, max_treedepth = 15),
                                      save_pars = save_pars(all = TRUE),
                                      seed = 187)
```

## 7 - Model Comparison
```{r model comparison, cache=TRUE, eval=FALSE, echo=FALSE}
loo1 <- brms::loo(only_population_intercept.fit, save_psis = TRUE, cores = 4, reloo = T)
loo2 <- brms::loo(population_all.fit, save_psis = TRUE, cores = 4, reloo = T)
loo3 <- brms::loo(population_all_hs.fit, save_psis = TRUE, cores = 4, reloo = T)
loo4 <- brms::loo(population_and_group_intercept.fit, save_psis = TRUE, cores = 4, reloo = T)
loo5 <- brms::loo(multi_hier1_title_post.fit, save_psis = TRUE, cores = 4, reloo = T)
loo6 <- brms::loo(multi_hier2_pClass_post.fit, save_psis = TRUE, cores = 4, reloo = T)
loo7 <- brms::loo(population_all_interactions.fit, save_psis = TRUE, cores = 4, reloo = T)
loo8 <- brms::loo(multi_hier1_title_post_interactions.fit, save_psis = TRUE, cores = 4, reloo = T)
loo9 <- brms::loo(multi_hier2_pClass_post_interactions.fit, save_psis = TRUE, cores = 4, reloo = T)
loo10 <- brms::loo(multi_hier3_doubleClass.fit, save_psis = TRUE, cores = 4, reloo = T)
loo11 <- brms::loo(multi_hier4_doubleClass.fit, save_psis = TRUE, cores = 4, reloo = T)
loo12 <- brms::loo(multi_hier5_doubleClass.fit, save_psis = TRUE, cores = 4, reloo = T)

print(loo1)
print(loo2)
print(loo3)
print(loo4)
print(loo5)
print(loo6)
print(loo7)
print(loo8)
print(loo9)
print(loo10)
print(loo11)
print(loo12)

plot(loo1, label_points = TRUE)
plot(loo2, label_points = TRUE)
plot(loo3, label_points = TRUE)
plot(loo4, label_points = TRUE)
plot(loo5, label_points = TRUE)
plot(loo6, label_points = TRUE)
plot(loo7, label_points = TRUE)
plot(loo8, label_points = TRUE)
plot(loo9, label_points = TRUE)
plot(loo10, label_points = TRUE)
plot(loo11, label_points = TRUE)
plot(loo12, label_points = TRUE)

brms::loo_compare(loo1,loo2,loo3,loo4,loo5,loo6,loo7,loo8,loo9,loo10,loo11,loo12)

# using list of loo objects
loo_list <- list(loo1,loo2,loo3,loo4,loo5,loo6,loo7,loo8,loo9,loo10, loo11,loo12)
loo_model_weights(loo_list)
```

## 8 - Prediction Submission
```{r predictions model1, cache=TRUE, eval=FALSE, echo=FALSE}
title <- titleTest
pClass <- pClassTest
ticket <- ticketTest
embarked <- embarkedTest
isMale <- isMaleTest

df.preds <- cbind(df0.test,title,pClass,ticket,embarked,isMale)

probs_all <- brms::pp_average(only_population_intercept.fit,population_all.fit,population_all_hs.fit,
                          population_and_group_intercept.fit,multi_hier1_title_post.fit,
                          multi_hier2_pClass_post.fit,population_all_interactions.fit,
                          multi_hier1_title_post_interactions.fit,multi_hier2_pClass_post_interactions.fit,
                          multi_hier3_doubleClass.fit,multi_hier4_doubleClass.fit,multi_hier5_doubleClass.fit,
                          newdata = df.preds, allow_new_levels = T)

preds_all <- ifelse(probs_all[,1] > 0.5, 1, 0)

model_all.submission <- data.frame("PassengerId" = pId.test, "Survived" = preds_all)
write.csv(x = model_all.submission, file = file.path("C:","Users","alpgu","Dropbox","Data Science",                                             "Bayesian Data Analysis","BDA3 (Gelman)","titanic","submissions","model_bma.csv"),
          row.names = FALSE)

# prediction with best predictive performance model according to loo
ticket <- ticketTest
embarked <- embarkedTest
df3.test <- as.data.frame(cbind(X3.test,ticket,embarked))

y_pred10<- posterior_predict(multi_hier3_doubleClass.fit, df3.test, allow_new_levels = T)
prob10 <- colMeans(y_pred10)
pred10 <- as.integer(prob10 >= 0.5)

model_doubleClass.submission <- data.frame("PassengerId" = pId.test, "Survived" = pred10)
write.csv(x = model_doubleClass.submission, file = file.path("C:","Users","alpgu","Dropbox","Data Science",                                             "Bayesian Data Analysis","BDA3 (Gelman)","titanic","submissions","model_doubleClass.csv"),
          row.names = FALSE)
```
